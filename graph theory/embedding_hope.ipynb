{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090123ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import terminé a : 10:22:59\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importation des librairies \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import progressbar\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score,precision_score,recall_score, precision_recall_fscore_support \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn_som.som import SOM\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from GEM.gem.utils      import graph_util, plot_util\n",
    "from GEM.gem.evaluation import visualize_embedding as viz\n",
    "from GEM.gem.evaluation import evaluate_graph_reconstruction as gr\n",
    "from GEM.gem.embedding.gf       import GraphFactorization\n",
    "#from GEM.gem.embedding.sdne     import SDNE\n",
    "#from argparse import ArgumentParser\n",
    "#from GraphEmbedding.ge import DeepWalk\n",
    "#from GraphEmbedding.ge import SDNE\n",
    "from karateclub.graph_embedding import Graph2Vec\n",
    "from karateclub.node_embedding.neighbourhood import HOPE\n",
    "from karateclub.node_embedding.neighbourhood import DeepWalk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_frame=200#arbitraire , a tester plus serieusement\n",
    "\n",
    "print(\"import terminé a :\",time.strftime(\"%H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fb8668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fonctions declaré a : 10:22:59\n"
     ]
    }
   ],
   "source": [
    "def instals():\n",
    "    !pip install progressbar2\n",
    "    !pip3 install PyQt5\n",
    "    \n",
    "    \n",
    "\n",
    "def create_graph(data_set):\n",
    "    g = nx.MultiGraph()\n",
    "\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while (i<len(data_set)):\n",
    "        a=data_set[\"merchant\"][i]\n",
    "        b=data_set[\"cc_num\"][i]\n",
    "        g.add_edge(a,b,weight=1,edge_id=i)\n",
    "        i=i+1\n",
    "    \n",
    "    print(\"---graph construction = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return g\n",
    "\n",
    "def create_graph_deprecated(X_train_ultra_simple):\n",
    "    g = nx.MultiGraph()\n",
    "\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while (i<len(X_train_ultra_simple)):\n",
    "        a=X_train_ultra_simple[\"merchant\"][i]\n",
    "        b=X_train_ultra_simple[\"cc_num\"][i]\n",
    "        g.add_edge(a,b,weight=0,edge_id=i)\n",
    "        i=i+1\n",
    "    i=0\n",
    "    while (i<len(X_train_ultra_simple)):# on lis 2 fois mais ca coute que 4 sec\n",
    "        a=X_train_ultra_simple[\"merchant\"][i]\n",
    "        b=X_train_ultra_simple[\"cc_num\"][i]\n",
    "        g[a][b][\"weight\"]=g[a][b][\"weight\"]+1 # il falais initialiser en premier\n",
    "        i=i+1\n",
    "    \n",
    "    print(\"---graph construction = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return g\n",
    "\n",
    "def fill(g,liste):\n",
    "  \n",
    "    while(liste):\n",
    "        a=liste.pop()\n",
    "        i=0\n",
    "        while(i<len(liste)):\n",
    "            b=liste[i]\n",
    "            g.add_edge(a,b,weight=1)\n",
    "            i+=1\n",
    "    \n",
    "    #return g\n",
    "\n",
    "def init_sub_graph(nb_frames):\n",
    "    # divison en plusieures sous graphs \n",
    "    sous_graph=[]\n",
    "    i=0\n",
    "    i2=0\n",
    "    sub_g=0\n",
    "    while ( i<nb_frames):\n",
    "        sous_graph.append(nx.MultiGraph())\n",
    "        i=i+1\n",
    "    return sous_graph\n",
    "\n",
    "def bipartite_dict(dict_merchants,dict_cc_num):\n",
    "\n",
    "    dict_merchants_copy=dict_merchants.copy()\n",
    "    dict_merchants_copy = dict([(value, key) for key, value in dict_merchants_copy.items()])\n",
    "    dict_cc_num_copy=dict_cc_num.copy()\n",
    "    dict_cc_num_copy = dict([(value, key) for key, value in dict_cc_num_copy.items()])\n",
    "\n",
    "    for key in dict_merchants_copy.keys():\n",
    "        dict_merchants_copy[key] = 0\n",
    "    for key in dict_cc_num_copy.keys():\n",
    "        dict_cc_num_copy[key] = 1\n",
    "    return dict_merchants_copy,dict_cc_num_copy\n",
    "\n",
    "def create_sub_graph(data_set,nb_frames,dict_merchants,dict_cc_num,start_at=0):\n",
    "    sub_g=init_sub_graph(nb_frames)\n",
    "    print (len(sub_g),\" sous graphs\")\n",
    "    #sg in sub_g\n",
    "    dict_merchants_copy,dict_cc_num_copy=bipartite_dict(dict_merchants,dict_cc_num)    \n",
    "    time_frame_size=len(data_set) / nb_frames\n",
    "    start = time.time()\n",
    "    connected_count=0\n",
    "    i=start_at\n",
    "    data_set_size=len(data_set)+i\n",
    "    i2=0\n",
    "    sg=0\n",
    "    while (i<data_set_size):\n",
    "        a=data_set[\"merchant\"][i]# rendre plus lisible\n",
    "        b=data_set[\"cc_num\"][i]\n",
    "        sub_g[sg].add_edge(a,b,weight=1,edge_id=i,edge_rid=i2)\n",
    "\n",
    "        i=i+1\n",
    "        i2=i2+1\n",
    "        if i2>= time_frame_size:\n",
    "            nx.set_node_attributes(sub_g[sg], dict_merchants_copy, \"bipartite\")\n",
    "            nx.set_node_attributes(sub_g[sg], dict_cc_num_copy, \"bipartite\")\n",
    "            i2=0\n",
    "            if(nx.is_connected(sub_g[sg])):\n",
    "                connected_count+=1\n",
    "            sg=sg+1\n",
    "\n",
    "    print(\"---graph split = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "    return sub_g,connected_count\n",
    "\n",
    "\n",
    "def create_sub_graph_deprecated(g,nb_frames,dict_merchants,dict_cc_num):\n",
    "    sous_graph=init_sub_graph(nb_frames)\n",
    "    dict_merchants_copy,dict_cc_num_copy=bipartite_dict(dict_merchants,dict_cc_num)    \n",
    "    time_frame_size=len(X_train_ultra_simple) / num_frame\n",
    "    start = time.time()\n",
    "    connected_count=0\n",
    "    i=0\n",
    "    i2=0\n",
    "    sub_g=0\n",
    "    while (i<len(X_train_ultra_simple)):\n",
    "        a=X_train_ultra_simple[\"merchant\"][i]# rendre plus lisible\n",
    "        b=X_train_ultra_simple[\"cc_num\"][i]\n",
    "        if(sous_graph[sub_g].has_edge(a,b)):\n",
    "            sous_graph[sub_g][a][b][\"weight\"]=sous_graph[sub_g][a][b][\"weight\"]+1 \n",
    "        else:\n",
    "            sous_graph[sub_g].add_edge(a,b,weight=1,edge_id=i)\n",
    "            #sous_graph[sub_g][a][\"bipartite\"]=0\n",
    "            #sous_graph[sub_g][b][\"bipartite\"]=1\n",
    "        i=i+1\n",
    "        i2=i2+1\n",
    "        if i2>= time_frame_size:\n",
    "            nx.set_node_attributes(sous_graph[sub_g], dict_merchants_copy, \"bipartite\")\n",
    "            nx.set_node_attributes(sous_graph[sub_g], dict_cc_num_copy, \"bipartite\")\n",
    "            i2=0\n",
    "            if(nx.is_connected(sous_graph[sub_g])):\n",
    "                connected_count+=1\n",
    "            sub_g=sub_g+1\n",
    "\n",
    "    print(\"---graph split = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "    return sous_graph,connected_count\n",
    "\n",
    "\n",
    "def invert_graph(gr):\n",
    "    new_graph=nx.MultiGraph()\n",
    "    #tous les arc d'un sommet sont connecté entre eux\n",
    "    #step 1 = dans new_graph creer un sommet pour chaque arc\n",
    "\n",
    "    for edge in gr.edges(data=True):\n",
    "        #print(edge)\n",
    "        node_id=edge[2][\"edge_rid\"]\n",
    "        _edge_id=edge[2][\"edge_id\"]\n",
    "        new_graph.add_node(node_id,edge_id=_edge_id)\n",
    "\n",
    "    #pour chaque node de gr relier enssemble tous les arc\n",
    "    for node in gr.nodes:\n",
    "        _edge_list=list(gr.edges(node,data=\"edge_rid\"))\n",
    "        node_list=[]\n",
    "        i=0\n",
    "        #print(_edge_list)\n",
    "        for items in _edge_list:\n",
    "            #print(node[0])\n",
    "            node_list.append(_edge_list[i][2])\n",
    "            i=i+1            \n",
    "        fill(new_graph,node_list)\n",
    "\n",
    "\n",
    "    return new_graph\n",
    "\n",
    "def invert_graph_deprecated(gr):\n",
    "    new_graph=nx.MultiGraph()\n",
    "    #tous les arc d'un sommet sont connecté entre eux\n",
    "    #step 1 = dans new_graph creer un sommet pour chaque arc\n",
    "\n",
    "    for edge in gr.edges(data=True):\n",
    "        #print(edge)\n",
    "        node_id=edge[2][\"edge_rid\"]\n",
    "        new_graph.add_node(node_id)\n",
    "\n",
    "        #print(\" g[edge[0]][edge[1]][edge_id] = \",g[edge[0]][edge[1]][\"edge_id\"])\n",
    "    #pour chaque node de g relier enssemble tous les arc\n",
    "    for node in gr.nodes:\n",
    "        dico=list(gr[node])# traitement de 1 node\n",
    "        #print(dico)\n",
    "        node_list=[]\n",
    "        for key in dico:\n",
    "            \n",
    "            node_id=gr[node][key][0][\"edge_rid\"]# to test\n",
    "            node_list.append(node_id)\n",
    "        fill(new_graph,node_list)\n",
    "\n",
    "\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "def create_inverted_sub_graph(sub_g):\n",
    "    \n",
    "    \n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(sub_g))\n",
    "\n",
    "    i=0\n",
    "    start = time.time\n",
    "    inv_sous_graph=[]\n",
    "    for sg in sub_g:\n",
    "        inv_sous_graph.append(invert_graph(sg))\n",
    "        i=i+1\n",
    "        process_bar.update(i)\n",
    "        \n",
    "    return inv_sous_graph\n",
    "    print(\"---create_inverted_sub_graph  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "\n",
    "\n",
    "\n",
    "def start_time_eval():\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while(i<1000000):\n",
    "        i=i+1\n",
    "    boucle_time=time.time() - start\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while(i<1000000):\n",
    "        poubelle =time.time()\n",
    "        i=i+1\n",
    "    print(\"---1 milion de time.time=  %s seconds ---\" % (time.time() - start-boucle_time));start = time.time()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#return le nombre d'arete du graph weighted_g\n",
    "def nb_edge(weighted_g):\n",
    "    summ=0\n",
    "    NODES = list(weighted_g.nodes)\n",
    "    for node in NODES:\n",
    "        summ= summ+G.degree[node]\n",
    "    return summ/2\n",
    "\n",
    "def replissement(weighted_g, nb_merc,nb_cc_num):\n",
    "    nb_edges=nb_edge(weighted_g)\n",
    "    nb_max_edges =(nb_merc* nb_cc_num)#graph bipati\n",
    "    return  nb_edges / nb_max_edges\n",
    "    \n",
    "def edge_repartition(g,len_dict_merchants):\n",
    "    repartition=[]\n",
    "    #remplissage de repartiotion avec 0 pour eviter les bugg\n",
    "    \n",
    "    #for each vertex \n",
    "        # for each edge in vertex.edges\n",
    "            #repartition[ edge.poid ] ++\n",
    "    print (\"\")\n",
    "    \n",
    "\n",
    "def slow_concat(d1,d2):\n",
    "    return dict(d1.items() | d2.items())\n",
    "\n",
    "def ditc_maping_so_slow_but_why(X_train_ultra_simple,dict_merchants,dict_cc_num):\n",
    "    #---dictionary maping = 4272.313026428223 seconds ---\n",
    "    start = time.time()\n",
    "    size =len(X_train_ultra_simple)\n",
    "    i=0\n",
    "    time_val=[]\n",
    "    while (i<size):\n",
    "        X_train_ultra_simple.iat[i,0]=dict_merchants[X_train_ultra_simple.iat[i,0]]\n",
    "        X_train_ultra_simple.iat[i,1]=dict_cc_num[X_train_ultra_simple.iat[i,1]]\n",
    "        time_val.append(time.time() - start);start = time.time()\n",
    "        i=i+1\n",
    "    return time_val\n",
    "\n",
    "\n",
    "def ditc_maping(X_train_ultra_simple,dict_merchants_cc_num):\n",
    "    X_train_ultra_simple[\"merchant\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "    X_train_ultra_simple[\"cc_num\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "\n",
    "def ditc_maping_2(X_train_ultra_simple,dict_merchants_cc_num):\n",
    "    X_train_ultra_simple[\"merchant\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "    X_train_ultra_simple[\"cc_num\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "\n",
    "    \n",
    "def create_split_dict(X_train_ultra_simple,start_at=0):\n",
    "    index=start_at\n",
    "    start = time.time()\n",
    "    dict_merchants=dict()\n",
    "    dict_cc_num=dict()\n",
    "    merc_id=0\n",
    "    cc_id=0\n",
    "    data_set_size= len(X_train_ultra_simple)+index\n",
    "    while index < data_set_size:\n",
    "        if X_train_ultra_simple[\"merchant\"][index] not in dict_merchants.keys():\n",
    "            dict_merchants[X_train_ultra_simple[\"merchant\"][index]] = merc_id\n",
    "            merc_id=merc_id+1\n",
    "        index=index+1\n",
    "\n",
    "    print(\"---remplissage dict_merchants  %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    index=start_at\n",
    "    while index < data_set_size:\n",
    "        if X_train_ultra_simple[\"cc_num\"][index] not in dict_cc_num.keys():\n",
    "            dict_cc_num[X_train_ultra_simple[\"cc_num\"][index]] = merc_id\n",
    "            merc_id=merc_id+1\n",
    "        index=index+1\n",
    "\n",
    "    print(\"---remplissage dict_cc_num %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return dict_merchants,dict_cc_num\n",
    "\n",
    "def create_dict(data_set):\n",
    "    dict_merchants,dict_cc_num=create_split_dict(data_set)\n",
    "    \n",
    "    return slow_concat(dict_merchants,dict_cc_num)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "def create_dictionary_hell():# missing double values\n",
    "    start = time.time()\n",
    "    dictionary_hell=[]\n",
    "    nodes=[]\n",
    "    nb_sg=len(sous_graph)\n",
    "    print(len(sous_graph))\n",
    "    sg_index=0\n",
    "    merc_id=0\n",
    "    while sg_index < nb_sg:\n",
    "        i=0\n",
    "        merc_id=0\n",
    "        nodes= list(sous_graph[sg_index].nodes)\n",
    "        dictionary_hell.append(dict())\n",
    "        while( i<len(nodes)):\n",
    "            if nodes[i] not in dictionary_hell[sg_index].keys():\n",
    "                dictionary_hell[sg_index][nodes[i]] = merc_id\n",
    "                merc_id=merc_id+1\n",
    "                \n",
    "            i=i+1\n",
    "\n",
    "        sg_index=sg_index+1\n",
    "    print(\"--- dictionary__hell %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return dictionary_hell\n",
    "\n",
    "def relabel_graph(r_sous_graph,dictionary_hell,sous_graph):\n",
    "    print (\"relabel_\",end=\"\")\n",
    "    start = time.time()\n",
    "    i=0  \n",
    "    while i<len(sous_graph):\n",
    "        r_sous_graph.append(nx.relabel_nodes(sous_graph[i], dictionary_hell[i],copy=True))\n",
    "        i=i+1\n",
    "        print(\".\",end=\"\")\n",
    "    print(\"--- relabel_graph %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "def unrelabel_graph(unr_sous_graph,dictionary_hell,sous_graph):\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while i<len(dictionary_hell):\n",
    "        if i==len(sous_graph):\n",
    "            (print(\"errno=\",i,end=','))\n",
    "        inv_map = {v: k for k, v in dictionary_hell[i].items()}\n",
    "        unr_sous_graph.append(nx.relabel_nodes(sous_graph[i], inv_map,copy=True))\n",
    "        i=i+1\n",
    "    print(\"--- revert labelling %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "\n",
    "def print_info_diverses(X_train_ultra_simple,dico):\n",
    "    print(X_train_ultra_simple[\"merchant\"][0])\n",
    "    print(X_train_ultra_simple[\"cc_num\"][0])\n",
    "    print(X_train_ultra_simple.columns)\n",
    "    print(X_train_ultra_simple.loc[0][\"merchant\"])\n",
    "\n",
    "    print (len(dico) , \"humans in the system \")#1676\n",
    "    print(X_train_ultra_simple[\"cc_num\"][0])\n",
    "    print (dict_merchants[ \"fraud_Rippin, Kub and Mann\"], type(dico[ \"fraud_Rippin, Kub and Mann\"]))\n",
    "    \n",
    "\n",
    "#laplacian similarity 1/2\n",
    "def select_k(spectrum, minimum_energy = 0.9):#\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)\n",
    "\n",
    "#laplacian similarity 2/2\n",
    "def laplacian_similarity(graph1,graph2):\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "    print(\"k selected =\",k)\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    return similarity\n",
    "\n",
    "def string_edit_dist():\n",
    "    print(\"https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Levenshtein.html\")\n",
    "def edit_dist_nx(g1,g2):\n",
    "    \n",
    "    for v in nx.optimize_graph_edit_distance(g1, g2):\n",
    "        minv = v\n",
    "    return minv\n",
    "\n",
    "\n",
    "def graph_degree(g):\n",
    "    degrees = [val for (node, val) in g.degree()]\n",
    "    maxd=max(degrees)\n",
    "    avg=sum(degrees)/len(degrees)\n",
    "    #print(\"max degree =\",maxd,\" mean = \",avg)\n",
    "    return maxd,avg\n",
    "def print_graph_info(connected_count):\n",
    "    \n",
    "    print(connected_count,\"connected graphs\")\n",
    "    print(len(sous_graph[0].edges),\" transactions \")\n",
    "    print(\"sous_graph[0][0] , type = \",type(sous_graph[0][0]),\"\\n\")\n",
    "    print(\"sous_graph[0].nodes , type = \",type(sous_graph[0].nodes),\"\\n\")\n",
    "    print(list(sous_graph[0].nodes))\n",
    "    print(\"------------\")\n",
    "    sub_g=0;node=0\n",
    "    dico=dict(sous_graph[sub_g][node])\n",
    "    for key in dico:\n",
    "         print(sous_graph[0][0][key],\"key = \",key)\n",
    "    print(dict(sous_graph[0][0]))\n",
    "    print(\"------------\")\n",
    "    print( type(sous_graph[0][0][693]))\n",
    "    print(sous_graph[0][0][693])\n",
    "    print(\"---------\")\n",
    "    print((sous_graph[0].edges))\n",
    "    \n",
    "def draw_1(g):\n",
    "    start = time.time()\n",
    "    #subax1 = plt.subplot()\n",
    "    nx.draw(sous_graph[0], with_labels=False, node_size= 1)\n",
    "    plt.savefig(\"draw_1.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "def draw_2(g):\n",
    "    start = time.time()\n",
    "    #subax2 = plt.subplot()\n",
    "    options = {\n",
    "        'node_size': 100,\n",
    "        'width': 3,\n",
    "    }\n",
    "    nx.draw_spectral(g, **options)#approximation of the ratio cut\n",
    "    plt.savefig(\"draw_2.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "def draw_3(g):\n",
    "    start = time.time()\n",
    "    #subax3 = plt.subplot()\n",
    "\n",
    "    nx.draw_shell(g, with_labels=False,node_size= 1)# font_weight='bold')\n",
    "    plt.savefig(\"draw_3.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "    \n",
    "def draw_4(g,numb_merchant):\n",
    "    total=len(g.nodes)#les valeures ascossié ne sont pas les bonnes mais c'est \n",
    "    X = list(range(0,numb_merchant ))# juste pour la position geographique \n",
    "    Y= list(range(numb_merchant,total ))\n",
    "    pos = dict()\n",
    "    pos.update( (n, (1, i)) for i, n in enumerate(X) ) # put nodes from X at x=1\n",
    "    pos.update( (n, (2, i)) for i, n in enumerate(Y) ) # put nodes from Y at x=2\n",
    "    nx.draw(g, pos=pos,node_size= 1)\n",
    "    plt.savefig(\"draw_4.png\")\n",
    "    plt.show()\n",
    "\n",
    "def drawing(g):\n",
    "    %matplotlib inline\n",
    "    draw_1(g)\n",
    "    draw_2(g)\n",
    "    draw_3(g)\n",
    "    draw_4(g,len(dict_merchants.keys()))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def complexity_calculations():\n",
    "    val=0\n",
    "    train_size=len(Y_train)\n",
    "    test_size=len(Y_test)\n",
    "    \n",
    "    #l'idee premiere\n",
    "    print(\"-----------------\")\n",
    "    print(\"tester la similarité de 1 graph modifié avec tt les autres \")\n",
    "    print(\"pour 0.5 sec par calcul et \",num_frame,\" frames\" )\n",
    "    print( num_frame*0.5,\" sec\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"modifier le graph et recomencer , pour chaques valeures dans train\")\n",
    "    print(\"pour completement calculer  les similarité de 1 transaction\")\n",
    "    print( num_frame*0.5*train_size,\" sec\")\n",
    "    print(\"donc \",int(num_frame*0.5*train_size/(3600*24)),\" jours\")\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    big_number = num_frame*0.5*train_size*test_size\n",
    "    big_number_year=int(big_number/31540000)\n",
    "    print(\"pour un total de \",big_number,\" sec\")\n",
    "    print(\"donc \",big_number_year,\" ans\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"en reduisant la precision au minimum\")\n",
    "    print(\"chaque transaction n'aura que 1 calcul de similarité\")\n",
    "    print(\"precision max 50% , doubler le temps de calcul double la precision\")\n",
    "    big_number=0.5*test_size\n",
    "    print(\"pour un total de \",big_number,\" sec\")\n",
    "    print(\"donc \",int(big_number/3600),\" heures\")\n",
    "\n",
    "    \n",
    "def int_to_str(G):\n",
    "    # convert nodes from int to str format\n",
    "    keys = np.arange(0,int(len(dictionary.keys())))\n",
    "    values = [str(i) for i in keys]\n",
    "    dic = dict(zip(keys, values))\n",
    "    H = nx.relabel_nodes(G, dic)\n",
    "\n",
    "def try_catch_example():\n",
    "    print('How many cats do you have?\\n')\n",
    "    numCats = input()\n",
    "    try:\n",
    "        if int(numCats) > 3:\n",
    "            print('That is a lot of cats.')\n",
    "        else:\n",
    "            print('That is not that many cats.')\n",
    "    except ValueError:\n",
    "        print(\"Value error\")  \n",
    "def modif_graph_testing(graph):\n",
    "    graph[1][2][\"weight\"]=2\n",
    "    \n",
    "def invert_node_testing():\n",
    "    graph=nx.MultiGraph()\n",
    "    graph.add_nodes_from([0,1,2,3,4])\n",
    "    graph.add_edge(0,1,weight=0,edge_id=\"a\")\n",
    "    graph.add_edge(1,2,weight=0,edge_id=\"b\")\n",
    "    graph.add_edge(2,3,weight=0,edge_id=\"c\")\n",
    "    graph.add_edge(3,4,weight=0,edge_id=\"d\")\n",
    "    graph.add_edge(4,2,weight=0,edge_id=\"e\")\n",
    "    print(graph.nodes)\n",
    "    print(graph.edges)\n",
    "    print(\"avan 1--2--3--4\")\n",
    "    #new_graph=inver t_graph(graph)\n",
    "    print(\"apres 1--2--3--4\")\n",
    "    print(new_graph.nodes)\n",
    "    print(new_graph.edges)\n",
    "    \n",
    "    \n",
    "    \n",
    "def max_degree(inv_sg):\n",
    "    _max=0\n",
    "    for gr in inv_sg:\n",
    "        current_max,mean =graph_degree(gr)\n",
    "        if current_max>_max:\n",
    "            _max=current_max\n",
    "    print (\"max degree= \",_max)\n",
    "    return _max\n",
    "\n",
    "def print_version():\n",
    "    print (\"python \",sys.version)\n",
    "    import xgboost as xgb\n",
    "    print(\"xgboost version = \",xgb.__version__)\n",
    "\n",
    "\n",
    "def hope_on_inv_sg(inv_sg,dim=58,nb_frame=200):\n",
    "\n",
    "    start = time.time()\n",
    "    # hope un_attributed example\n",
    "    model = HOPE(dimensions=dim)\n",
    "    i=0\n",
    "    model.fit(inv_sg[i])\n",
    "    arr=model.get_embedding()\n",
    "    i=1\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=nb_frame)\n",
    "    while (i<nb_frame):\n",
    "        # train the model and generate embeddings\n",
    "        model.fit(inv_sg[i])\n",
    "        arr=np.concatenate((arr,model.get_embedding()),axis=0)\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---hope embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return arr  \n",
    "\n",
    "def hope_on_inv_sg_deprecated(inv_sg):\n",
    "\n",
    "    start = time.time()\n",
    "    # hope attributed example\n",
    "    model = HOPE(dimensions=58)\n",
    "    liste=[]\n",
    "    i=0\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=num_frame)\n",
    "    while (i<num_frame):\n",
    "        # train the model and generate embeddings\n",
    "        model.fit(inv_sg[i])\n",
    "        if(liste==[]):\n",
    "            emm=model.get_embedding()\n",
    "            print(type(emm))\n",
    "            liste.append(emm)\n",
    "        #missing else ??????????????????????\n",
    "        liste.extend(model.get_embedding())\n",
    "        #ar=np.concatenate((ar,ar2),axis=0)\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---hope embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return liste  \n",
    "\n",
    "def deepwalk_on_inv_sg(inv_sg,dim=58):\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    # hope un_attributed example\n",
    "    model = DeepWalk(dimensions=dim)\n",
    "    i=0\n",
    "    model.fit(inv_sg[i])\n",
    "    arr=model.get_embedding()\n",
    "    i=1\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=num_frame)\n",
    "    while (i<num_frame):\n",
    "        # train the model and generate embeddings\n",
    "        model.fit(inv_sg[i])\n",
    "        arr=np.concatenate((arr,model.get_embedding()),axis=0)\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---deepwalk embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return arr      \n",
    "\n",
    "def deepwalk_on_inv_sg_deprecated(inv_sg):\n",
    "\n",
    "    start = time.time()\n",
    "    # hope attributed example\n",
    "    model = DeepWalk(dimensions=58)\n",
    "    liste=[]\n",
    "    i=0\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=num_frame)\n",
    "    while (i<num_frame):\n",
    "        # train the model and generate embeddings\n",
    "        \n",
    "        model.fit(inv_sg[i])\n",
    "        if(liste==[]):\n",
    "            liste.append(model.get_embedding())\n",
    "        liste.extend(model.get_embedding())\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---deepwalk embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return liste      \n",
    "\n",
    "\n",
    "\n",
    "print(\"fonctions declaré a :\",time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2996dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431325d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long']\n",
      "(1037340, 2)\n",
      "(259335, 2)\n",
      "(555719, 2)\n"
     ]
    }
   ],
   "source": [
    "##download data\n",
    "\n",
    "program_start = time.time()\n",
    "\n",
    "data_file= os.path.abspath('../../data')\n",
    "full_path=data_file+'\\\\'+'fraudTrain.csv'\n",
    "train_df=pd.read_csv(full_path)\n",
    "full_path=data_file+'\\\\'+'fraudTest.csv'\n",
    "test_df=pd.read_csv(full_path)\n",
    "\n",
    "cols = train_df.columns.tolist()\n",
    "cols = [c for c in cols if c not in [\"is_fraud\"]]\n",
    "target = \"is_fraud\"\n",
    "print(cols)\n",
    "\n",
    "#Definition des nouvelles variables X_train and Y_train\n",
    "X_train = train_df[cols]\n",
    "Y_train = train_df[target]\n",
    "\n",
    "#Definition des nouvelles variables X_test and Y_test\n",
    "X_test = test_df[cols]\n",
    "Y_test = test_df[target]\n",
    "\n",
    "features = [ 'merchant', 'cc_num']\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]\n",
    "\n",
    "X_train_ultra_simple = X_train.copy()\n",
    "X_train_ultra_simple=X_train_ultra_simple.iloc[:int(len(X_train_ultra_simple)*0.8)]\n",
    "X_val_ultra_simple=X_train.copy().iloc[len(X_train_ultra_simple):]\n",
    "\n",
    "X_test_ultra_simple = X_test.copy()\n",
    "num_frame_test=round((len(X_test_ultra_simple)/len(X_train_ultra_simple))*num_frame)\n",
    "num_frame_val=round((len(X_val_ultra_simple)/len(X_train_ultra_simple))*num_frame)\n",
    "\n",
    "#you need to make a smaller number of subgraph\n",
    "print(X_train_ultra_simple.shape)\n",
    "print(X_val_ultra_simple.shape)\n",
    "print(X_test_ultra_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c91466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2882e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---remplissage dict_merchants  5.637519598007202 seconds ---\n",
      "---remplissage dict_cc_num 5.992034673690796 seconds ---\n",
      "---remplissage dict_merchants  1.483436107635498 seconds ---\n",
      "---remplissage dict_cc_num 1.53389310836792 seconds ---\n",
      "---remplissage dict_merchants  3.07381272315979 seconds ---\n",
      "---remplissage dict_cc_num 3.167191505432129 seconds ---\n",
      "---dictionary maping = 72.46729111671448 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#remplissage des dictionaires\n",
    "#_test\n",
    "\n",
    "dict_merchants_train,dict_cc_num=create_split_dict(X_train_ultra_simple)\n",
    "dict_merchants_val,dict_cc_num_val=create_split_dict(X_val_ultra_simple,start_at=len(X_train_ultra_simple))\n",
    "dict_merchants_test,dict_cc_num_test=create_split_dict(X_test_ultra_simple)\n",
    "dictionary=slow_concat(dict_merchants_train,dict_cc_num)\n",
    "dictionary_val=slow_concat(dict_merchants_val,dict_cc_num_val)\n",
    "dictionary_test=slow_concat(dict_merchants_test,dict_cc_num_test)\n",
    "\n",
    "#dictionary=create_dict(X_train_ultra_simple)\n",
    "\n",
    "# associer a chaque marchant son numero dans le dictionaire\n",
    "#pour la lisibilité , et l'affichage\n",
    "start = time.time()\n",
    "ditc_maping(X_train_ultra_simple,dictionary)\n",
    "ditc_maping(X_val_ultra_simple,dictionary_val)\n",
    "ditc_maping(X_test_ultra_simple,dictionary_test)\n",
    "print(\"---dictionary maping = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b895dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037340      0\n",
      "1037341      1\n",
      "1037342      2\n",
      "1037343      3\n",
      "1037344      4\n",
      "          ... \n",
      "1296670    564\n",
      "1296671    246\n",
      "1296672    646\n",
      "1296673    121\n",
      "1296674    353\n",
      "Name: merchant, Length: 259335, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_val_ultra_simple[\"merchant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d79b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200  sous graphs\n",
      "---graph split = 17.942409992218018 seconds ---\n",
      "50  sous graphs\n",
      "---graph split = 4.491936206817627 seconds ---\n",
      "107  sous graphs\n",
      "---graph split = 9.022253513336182 seconds ---\n",
      "85  connected graphs\n",
      "---depuis le debut  132.8589255809784 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#pip install tk\n",
    "\n",
    "    ##creation du graph\n",
    "    \n",
    "#g = create_graph(X_train_ultra_simple)#40 sec\n",
    "#g_val=create_graph(X_val_ultra_simple)\n",
    "#g_test= create_graph(X_test_ultra_simple)\n",
    "#!jupyter notebook --generate-config\n",
    "# divison en plusieures sous graphs #20 sec\n",
    "\n",
    "sous_graph_train,connected_count=create_sub_graph(X_train_ultra_simple,num_frame,dict_merchants_train,dict_cc_num)\n",
    "sous_graph_val,connected_count=create_sub_graph(X_val_ultra_simple,num_frame_val,dict_merchants_train,dict_cc_num,start_at=len(X_train_ultra_simple))\n",
    "sous_graph_test,connected_count=create_sub_graph(X_test_ultra_simple,num_frame_test,dict_merchants_train,dict_cc_num)\n",
    "\n",
    "#dictionary_hell=create_dictionary_hell()\n",
    "#pour afficher , attention au cascades\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "print(connected_count,\" connected graphs\")\n",
    "\n",
    "print(\"---depuis le debut  %s seconds ---\" % (time.time() - program_start));start = time.time\n",
    "#!git clone https://github.com/shenweichen/GraphEmbedding.git\n",
    "\n",
    "\n",
    "if(False):\n",
    "    liste=[]\n",
    "    i=0\n",
    "    num_frame=len(sous_graph)\n",
    "    G = sous_graph[i]\n",
    "    liste.append(encapsulation(G))\n",
    "    i+=1\n",
    "\n",
    "    G = sous_graph[i]\n",
    "    liste.append(encapsulation(G))\n",
    "    i+=1\n",
    "\n",
    "#crash_free()\n",
    "#!pip3 install PyQt5\n",
    "# affichage\n",
    "\n",
    "#drawing(sous_graph[0])\n",
    "\n",
    "\n",
    "#370 MiB = 300 000 Mo\n",
    "# avec g/200 ca compile , tres .... lentement , mais ca compile\n",
    "too_long=True\n",
    "if(not too_long):\n",
    "    val = edit_dist_nx(sous_graph[0],sous_graph[1])\n",
    "\n",
    "affichage=False\n",
    "if(affichage):\n",
    "    complexity_calculations()\n",
    "    drawing(g)\n",
    "    print_graph_info(connected_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb73f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging=False\n",
    "if (debugging):\n",
    "    sous_graph,connected_count=create_sub_graph(X_train_ultra_simple,num_frame,dict_merchants,dict_cc_num)\n",
    "    sous_graph_test,connected_count=create_sub_graph(X_test_ultra_simple,num_frame_test,dict_merchants,dict_cc_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b81799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "107\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "print(num_frame)\n",
    "print(len(sous_graph_train))\n",
    "print(len(sous_graph_test))\n",
    "print(num_frame_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6a91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### partie karateclub\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (107 of 107) |######################| Elapsed Time: 0:00:22 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- graph inversion = 69 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#graph construction\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "inv_sg_train= create_inverted_sub_graph(sous_graph_train)\n",
    "inv_sg_val = create_inverted_sub_graph(sous_graph_val)\n",
    "inv_sg_test= create_inverted_sub_graph(sous_graph_test)\n",
    "print(\"--- graph inversion = %s seconds ---\" % int(time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b06d26a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_it_missing_a_number(whatever):\n",
    "    missing=False\n",
    "    if isinstance(whatever,dict):\n",
    "    \n",
    "        l=sorted(whatever.keys())\n",
    "        i=0\n",
    "        while i<len(l):\n",
    "            if l[i]!=i:\n",
    "                missing=True\n",
    "            i=i+1\n",
    "        i=i-1\n",
    "        #print(\"len(dict)= \",len(l),\" , lastval = \",l[i],end=\" \")\n",
    "        if(missing):\n",
    "            print(\" missing a number\",end=\" \")\n",
    "            return True\n",
    "    if isinstance(whatever,list):\n",
    "        l=sorted(whatever)\n",
    "        i=0\n",
    "        while i<len(l):\n",
    "            if l[i]!=i:\n",
    "                missing=True\n",
    "            i=i+1\n",
    "        i=i-1\n",
    "        #print(\"len(list)= \",len(l),\" , lastval = \",l[i],end=\" \")\n",
    "        if(missing):\n",
    "            print(\" missing a number\",end=\" \")\n",
    "            return True\n",
    "    #print(\"\\n\")\n",
    "    return False\n",
    "    \n",
    "def is_inv_working():\n",
    "    print (len(inv_sg[30].nodes))\n",
    "    l1=list(inv_sg[30].nodes)\n",
    "    l2=list(unr_sous_graph[30].nodes)\n",
    "    i=0\n",
    "    while(i<len( inv_sg[30])):\n",
    "        if(l1[i] != l2[i]):\n",
    "            print(\"not_the_same\")\n",
    "        i=i+1\n",
    "\n",
    "    i=0# il y a au moins les memes values\n",
    "    for k in inv_sg[30] :\n",
    "        #print(\",\",end=\"\");i=i+1\n",
    "        if k not in(unr_sous_graph[30] ):\n",
    "            print(\"not_the_same\")\n",
    "    print(i)\n",
    "    \n",
    "def is_inv_conservating_order(inv_sg):\n",
    "    node_list=[]\n",
    "    for node in inv_sg.nodes(data=True):\n",
    "        \n",
    "        node_list.append([node[0],node[1][\"edge_id\"]])\n",
    "        #[inv_sg.nodes(node,data=\"node_id\"),inv_sg.nodes(node,data=\"edge_id\")]\n",
    "    #print (np.array(node_list))\n",
    "    return (np.array(node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78412f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead262ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max degree=  48\n",
      "max degree=  48\n",
      "max degree=  47\n",
      "200\n",
      "107\n",
      "len inv_sg =  200 1037400  / len xtrain =  1037340\n",
      "len inv_sg_val =  50  / len xval =  259335\n",
      "len inv_sg_test =  107  / len xtest =  555719\n"
     ]
    }
   ],
   "source": [
    "#node_list=is_inv_conservating_order(inv_sg_train[10])\n",
    "#node_list.sort(axis=0)\n",
    "#print(node_list)\n",
    "\n",
    "md=max_degree(inv_sg_train)\n",
    "md2=max_degree(inv_sg_val)\n",
    "md3=max_degree(inv_sg_test)\n",
    "print (len(inv_sg_train))\n",
    "print (len(inv_sg_test))#should be 107\n",
    "print(\"len inv_sg = \",len(inv_sg_train),len(inv_sg_train)*len(inv_sg_train[0]),\" / len xtrain = \",len(X_train_ultra_simple))\n",
    "print(\"len inv_sg_val = \",len(inv_sg_val),\" / len xval = \",len(X_val_ultra_simple))\n",
    "print(\"len inv_sg_test = \",len(inv_sg_test),\" / len xtest = \",len(X_test_ultra_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403819e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f71190ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(list(r_sous_graph[0].nodes)))\n",
    "#print(sorted((dictionary_hell[0]).keys()))\n",
    "#is_it_missing_a_number(dictionary_hell[1])\n",
    "i=0\n",
    "while(i<len(inv_sg_train)):\n",
    "    nodes = inv_sg_train[i].nodes(data=False)\n",
    "    is_it_missing_a_number(list(nodes))\n",
    "    i=i+1\n",
    "# Graph2Vec attributed example\n",
    "if(False):\n",
    "    small_sous_graph=[r_sous_graph[0],r_sous_graph[1]]\n",
    "    \n",
    "    model = Graph2Vec(attributed=False)\n",
    "    model.fit(small_sous_graph)\n",
    "    ecmb=model.get_embedding()\n",
    "\n",
    "    print(type(range(r_sous_graph[0].number_of_nodes())))\n",
    "\n",
    "    print(\"---embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76816aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max degree=  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (200 of 200) |######################| Elapsed Time: 0:02:04 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---hope embedding = 124.71520638465881 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (50 of 50) |########################| Elapsed Time: 0:00:36 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---hope embedding = 37.264121770858765 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (107 of 107) |######################| Elapsed Time: 0:01:05 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---hope embedding = 66.31422781944275 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#---hope embedding = 354.7910692691803 seconds ---\n",
    "md=max_degree(inv_sg_train)\n",
    "emb_train = hope_on_inv_sg(inv_sg_train,dim=md,nb_frame=num_frame)\n",
    "emb_val = hope_on_inv_sg(inv_sg_val,dim=md,nb_frame=num_frame_val)\n",
    "\n",
    "emb_test = hope_on_inv_sg(inv_sg_test,dim=md,nb_frame=num_frame_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96586af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e224bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---deepwalk embedding = 6973.2731170654297 seconds ---\n",
    "#deepwalk_emb=deepwalk_on_inv_sg(inv_sg)\n",
    "#deepwalk_emb=deepwalk_on_inv_sg(inv_sg,max_degree(inv_sg))# variable size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675a616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613f992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e461b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ded700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- fin de la partie graph\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "#-----------------   debut de la partie IA\n",
    "#  xgboost lstm random_forest neural_network\n",
    "# svm ---> +70 heure d'exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8f758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f6f5d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['merchant', 'category', 'amt', 'gender', 'state', 'zip', 'lat', 'long', 'city_pop', 'dob', 'unix_time', 'merch_lat', 'merch_long', 'delta_time', 'delta_amt', 'delta_time_category', 'delta_amt_category', 'delta_time_merchant', 'delta_amt_merchant', 'avg_amt', 'delta_avg_amt', 'avg_amt_category', 'delta_avg_amt_category', 'avg_amt_merchant', 'avg_amt_state', 'avg_amt_city', 'avg_amt_job', 'delta_avg_amt_category_job', 'month', 'day', 'hour']\n"
     ]
    }
   ],
   "source": [
    "#import le data_set complet pour xgboost et cie\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_train_1_2_svm.csv'\n",
    "xtrain_transformed_complique=pd.read_csv(full_path)\n",
    "ytrain_transformed_complique=train_df['is_fraud'].iloc[:int(len(train_df)*0.8)]\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_val_1_2_svm.csv'\n",
    "xval_transformed_complique=pd.read_csv(full_path)\n",
    "yval_transformed_complique=train_df['is_fraud'].iloc[len(ytrain_transformed_complique):]\n",
    "\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_test_1_2_svm.csv'\n",
    "xtest_transformed_complique=pd.read_csv(full_path)\n",
    "ytest_transformed_complique=test_df['is_fraud']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df=train_df.drop(columns=['Unnamed: 0'])\n",
    "test_df=test_df.drop(columns=['Unnamed: 0'])\n",
    "xtrain_transformed_complique=xtrain_transformed_complique.drop(columns=['Unnamed: 0'])\n",
    "xval_transformed_complique=xval_transformed_complique.drop(columns=['Unnamed: 0'])\n",
    "xtest_transformed_complique=xtest_transformed_complique.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "cols = xtrain_transformed_complique.columns.tolist()\n",
    "print(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4526d6d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037340, 48)\n",
      "(1037340, 31)\n",
      "(1037340, 79)\n"
     ]
    }
   ],
   "source": [
    "#add columns\n",
    "print((emb_train.shape))\n",
    "print(xtrain_transformed_complique.shape)\n",
    "emb_xtrain_concat=np.concatenate((xtrain_transformed_complique,emb_train),axis=1)\n",
    "emb_xval_concat=np.concatenate((xval_transformed_complique,emb_val),axis=1)\n",
    "emb_xtest_concat=np.concatenate((xtest_transformed_complique,emb_test),axis=1)\n",
    "\n",
    "print(emb_xtrain_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac8e8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfd7aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin.marty\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:30:35] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       0.98      0.83      0.90      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       0.99      0.91      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.95      0.79      0.86      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.89      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "---XGboost sans modification = 81.52752661705017 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# sans modification\n",
    "start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=50, gamma=0.05,eta=0.05,max_depth=7, n_jobs=16)\n",
    "\n",
    "xgb.fit(xtrain_transformed_complique,ytrain_transformed_complique)\n",
    "Y_train_pred=xgb.predict(xtrain_transformed_complique)\n",
    "Y_test_pred=xgb.predict(xtest_transformed_complique)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred))\n",
    "print(classification_report(Y_test,Y_test_pred))\n",
    "\n",
    "print(\"---XGboost sans modification = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f65ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin.marty\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:32:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       0.98      0.83      0.90      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       0.99      0.91      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.95      0.79      0.86      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.89      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "---XGboost avec modification = 300.9893820285797 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#ajout du graph\n",
    "\n",
    "xgb.fit(emb_xtrain_concat,ytrain_transformed_complique)\n",
    "Y_train_pred_emb=xgb.predict(emb_xtrain_concat)\n",
    "Y_test_pred_emb=xgb.predict(emb_xtest_concat)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred_emb))\n",
    "print(classification_report(Y_test,Y_test_pred_emb))\n",
    "\n",
    "print(\"---XGboost avec modification = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339a42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):# surement plus de 4 h\n",
    "    import shap\n",
    "    start = time.time()\n",
    "    explainer = shap.TreeExplainer(xgb) \n",
    "    shap_values = explainer.shap_values(emb_xtest_concat) \n",
    "    shap.summary_plot(shap_values, emb_xtest_concat, plot_type=\"bar\")\n",
    "    print(\"---shap for XGboost = %s seconds ---\" % (time.time() - start));start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8022c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall diff = -0.0005\n",
      "f1-score diff = -0.0005\n"
     ]
    }
   ],
   "source": [
    "print(\"xgboost\")\n",
    "cl=classification_report(Y_test,Y_test_pred,output_dict=True)\n",
    "cl2=classification_report(Y_test,Y_test_pred_emb,output_dict=True)\n",
    "print(\"recall diff = %.4f\" % (cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"]))\n",
    "print(\"f1-score diff = %.4f\" % (cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7903dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['merchant', 'category', 'amt', 'gender', 'state', 'zip', 'lat', 'long',\n",
      "       'city_pop', 'dob', 'unix_time', 'merch_lat', 'merch_long', 'delta_time',\n",
      "       'delta_amt', 'delta_time_category', 'delta_amt_category',\n",
      "       'delta_time_merchant', 'delta_amt_merchant', 'avg_amt', 'delta_avg_amt',\n",
      "       'avg_amt_category', 'delta_avg_amt_category', 'avg_amt_merchant',\n",
      "       'avg_amt_state', 'avg_amt_city', 'avg_amt_job',\n",
      "       'delta_avg_amt_category_job', 'month', 'day', 'hour'],\n",
      "      dtype='object')\n",
      "(1037340,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_transformed_complique.columns)\n",
    "print(Y_train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39a77f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm\n",
    "#print((xtrain_transformed_complique_lstm.shape))\n",
    "#print((xtrain_transformed_complique_lstm.shape[1:]))\n",
    "#print(ytrain_transformed_complique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b482a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "it_works=\"it_definitly_doesnt_work\"\n",
    "if(it_works==\"yes_it_does\"):\n",
    "    from tensorflow.keras.layers import LSTM, Layer, Dense\n",
    "    from tensorflow.keras.layers import Masking\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import metrics\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    xtrain_transformed_complique_lstm=np.expand_dims(xtrain_transformed_complique, 1) \n",
    "    xval_transformed_complique_lstm=np.expand_dims(xval_transformed_complique, 1) \n",
    "    xtest_transformed_complique_lstm=np.expand_dims(xtest_transformed_complique, 1) \n",
    "\n",
    "\n",
    "    lstm = Sequential()\n",
    "    lstm.add(LSTM(50, input_shape=(xtrain_transformed_complique_lstm.shape[1:]), return_sequences = True))\n",
    "    #lstm.add(LSTM(50, input_shape=xtrain_transformed_complique.shape[1:], return_sequences = True))\n",
    "\n",
    "    lstm.add(Dense(1,activation='sigmoid'))\n",
    "    lstm.summary()\n",
    "    #compile and fit the model\n",
    "    earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=3, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    lstm.compile(optimizer = 'adam',loss = 'binary_crossentropy' ,metrics = ['accuracy', metrics.Precision(class_id=1), metrics.Recall(class_id=1)])\n",
    "\n",
    "\n",
    "    history = lstm.fit(xtrain_transformed_complique_lstm, ytrain_transformed_complique, verbose=1, validation_data = (xval_transformed_complique_lstm, yval_transformed_complique), epochs = 50, batch_size = BATCH_SIZE, shuffle = False, callbacks = [earlystopper])\n",
    "    Y_train_pred = lstm.predict_classes(xtrain_transformed_complique_lstm, batch_size=BATCH_SIZE)\n",
    "    Y_test_pred = lstm.predict_classes(xtest_transformed_complique_lstm, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(classification_report(ytrain_transformed_complique,Y_train_pred))\n",
    "    print(classification_report(Y_test,Y_test_pred))\n",
    "\n",
    "    print(\"---XGboost avec modification = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ca204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00a01569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a5d724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual f1 score \n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "#trouve le meilleur dict\n",
    "def find_best_dict(dict_list):\n",
    "    threshold=0.92\n",
    "    while(threshold>0.50):\n",
    "        tmp_list=list()\n",
    "        for di in dict_list:\n",
    "            precision=(di[\"1\"][\"precision\"])\n",
    "            if(precision>threshold):\n",
    "                tmp_list.append(di)\n",
    "        if(tmp_list):\n",
    "            newlist = sorted(tmp_list, key=lambda d: d[\"1\"]['f1-score'],reverse=True)\n",
    "            print(\"choosen model precision: %.4f\" %newlist[0][\"1\"][\"precision\"],\" f1-score: %.4f\" %newlist[0][\"1\"]['f1-score'])\n",
    "            return newlist[0]\n",
    "        threshold=threshold-0.04\n",
    "    print(\"nothing good was found\")\n",
    "    return_value=sorted(dict_list, key=lambda d: d[\"1\"]['f1-score'],reverse=True)\n",
    "    return return_value[0]\n",
    "\n",
    "\n",
    "def fixed_nn_exec_time(xtrain_transformed_complique,ytrain_transformed_complique ,\n",
    "                        xtest_transformed_complique,ytest_transformed_complique ,\n",
    "                        tmps_exec=30,max_iter=1000):\n",
    "    start_function = time.time()\n",
    "    results=[]\n",
    "    curent_iter=0\n",
    "    while((tmps_exec*60)>(time.time() - start_function)):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_shape=(len(xtrain_transformed_complique.columns),),\n",
    "                        activation='relu')),\n",
    "        model.add(BatchNormalization()),\n",
    "        model.add(Dense(16, activation='relu')),\n",
    "        model.add(Dense(8, activation='relu')),\n",
    "        model.add(Dropout(0.2)),\n",
    "        model.add(Dense(4, activation='relu')),\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n",
    "        los=tf.keras.losses.BinaryCrossentropy()\n",
    "        model.compile(optimizer=opt, loss=\"binary_crossentropy\", \n",
    "                      metrics=[tf.keras.metrics.Precision(),\"accuracy\",f1])\n",
    "\n",
    "        history= model.fit(xtrain_transformed_complique, ytrain_transformed_complique\n",
    "                           ,epochs = 10, batch_size=128, verbose = 0)\n",
    "        history_dictict = history.history\n",
    "        y_test_pred =(model.predict(xtest_transformed_complique) >0.5).astype(\"int32\")\n",
    "        print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "\n",
    "        cl_nn=classification_report(ytest_transformed_complique,y_test_pred\n",
    "                                    ,output_dict=True)\n",
    "        results.append(cl_nn)\n",
    "\n",
    "        curent_iter=curent_iter+1\n",
    "        if curent_iter > max_iter:\n",
    "            find_best_dict(results)\n",
    "        \n",
    "    return find_best_dict(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecc6033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start nn no embeding\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.86      0.84      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.93      0.92      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.90      0.78      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.89      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.77      0.84      0.81      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.89      0.92      0.90    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.83      0.84      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.92      0.92      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.93      0.75      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.87      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.94      0.77      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.89      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.89      0.82      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.94      0.91      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.85      0.80      0.82      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.93      0.90      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.96      0.75      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.98      0.87      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.96      0.74      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.98      0.87      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.94      0.77      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.88      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.98      0.68      0.80      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.99      0.84      0.90    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.93      0.76      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.88      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.65      0.84      0.73      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.82      0.92      0.87    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.87      0.83      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.94      0.91      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "choosen model precision: 0.9443  f1-score: 0.8464\n",
      "---neural network = 2325.655211687088 seconds ---\n",
      "start nn with embeding\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.89      0.79      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.89      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.90      0.80      0.85      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.90      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.95      0.71      0.81      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.86      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.84      0.81      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.92      0.91      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.95      0.69      0.80      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.98      0.84      0.90    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.94      0.74      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.87      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.88      0.78      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.94      0.89      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.91      0.77      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.96      0.88      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.85      0.80      0.82      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.93      0.90      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.90      0.78      0.84      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.89      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.89      0.75      0.81      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.94      0.87      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.89      0.75      0.81      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.94      0.87      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.84      0.79      0.82      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.92      0.90      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.94      0.74      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.87      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.89      0.76      0.82      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.88      0.91    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "choosen model precision: 0.9433  f1-score: 0.8322\n",
      "---neural network = 1878.347571849823 seconds ---\n"
     ]
    }
   ],
   "source": [
    "nn_results=[]\n",
    "print(\"start nn no embeding\") \n",
    "nn_res=fixed_nn_exec_time(xtrain_transformed_complique,ytrain_transformed_complique ,\n",
    "                        xtest_transformed_complique,ytest_transformed_complique ,\n",
    "                        tmps_exec=30,max_iter=1000)\n",
    "nn_results.append(nn_res)\n",
    "print(\"---neural network = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "df_addon = pd.DataFrame(emb_train)\n",
    "X_train_plus_graph_df=pd.concat([xtrain_transformed_complique,df_addon],axis=1)\n",
    "\n",
    "df_addon = pd.DataFrame(emb_val)\n",
    "X_val_plus_graph_df=pd.concat([xval_transformed_complique,df_addon],axis=1)\n",
    "\n",
    "df_addon = pd.DataFrame(emb_test)\n",
    "X_test_plus_graph_df=pd.concat([xtest_transformed_complique,df_addon],axis=1)\n",
    "\n",
    "\n",
    "print(\"start nn with embeding\") \n",
    "nn_res=fixed_nn_exec_time(X_train_plus_graph_df,ytrain_transformed_complique ,\n",
    "                    X_test_plus_graph_df,ytest_transformed_complique ,\n",
    "                    tmps_exec=30)\n",
    "nn_results.append(nn_res)\n",
    "print(\"---neural network = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bb0c19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn recall diff = -0.0224\n",
      "nn f1-score diff = -0.0142\n"
     ]
    }
   ],
   "source": [
    "cl=nn_results[0]\n",
    "cl2=nn_results[1]\n",
    "print(\"nn recall diff = %.4f\" % (cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"]))\n",
    "print(\"nn f1-score diff = %.4f\" % (cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27603e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ea8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1f896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745d9180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b2eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b81dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aba4a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForest sans embedding = 249.9911766052246 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:  7.8min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForest avec embedding = 479.1933732032776 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_results=[]\n",
    "\n",
    "\n",
    "max_depth = 15\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=max_depth, min_samples_leaf= 3, min_samples_split= 2, n_estimators= 140, verbose=1, n_jobs=16)\n",
    "rf.fit(xtrain_transformed_complique, ytrain_transformed_complique)\n",
    "# Prediction for the training/validation set\n",
    "y_train_pred = rf.predict(xtrain_transformed_complique)\n",
    "y_test_pred = rf.predict(xtest_transformed_complique)\n",
    "rf_res=classification_report(ytest_transformed_complique,y_test_pred\n",
    "                            ,output_dict=True)\n",
    "rf_results.append(rf_res)\n",
    "print(\"---RandomForest sans embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "max_depth = 15\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=max_depth, min_samples_leaf= 3, min_samples_split= 2, n_estimators= 140, verbose=1, n_jobs=16)\n",
    "rf.fit(emb_xtrain_concat, ytrain_transformed_complique)\n",
    "# Prediction for the training/validation set\n",
    "y_train_pred_emb = rf.predict(emb_xtrain_concat)\n",
    "y_test_pred_emb = rf.predict(emb_xtest_concat)\n",
    "rf_res=classification_report(ytest_transformed_complique,y_test_pred_emb\n",
    "                            ,output_dict=True)\n",
    "rf_results.append(rf_res)\n",
    "print(\"---RandomForest avec embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1183e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       1.00      0.83      0.91      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       1.00      0.92      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.97      0.73      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.99      0.87      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       1.00      0.77      0.87      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       1.00      0.88      0.93   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.98      0.65      0.78      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.99      0.82      0.89    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrain_transformed_complique,y_train_pred))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(classification_report(ytrain_transformed_complique,y_train_pred_emb))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred_emb))\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "cl=rf_results[0]\n",
    "cl2=rf_results[1]\n",
    "print(\"rf recall diff = %.4f\" % (cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"]))\n",
    "print(\"rf f1-score diff = %.4f\" % (cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca718d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab0142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddecad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7d8554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc0122f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- svm fit = 2504.10399889946 seconds ---\n",
      "---SVM y_test_pred = 521.0672700405121 seconds ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.91      0.66      0.76      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.95      0.83      0.88    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'svm_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BENJAM~1.MAR\\AppData\\Local\\Temp/ipykernel_11716/3641780218.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m cl_svm=classification_report(ytest_transformed_complique,y_test_pred\n\u001b[0;32m     14\u001b[0m                             ,output_dict=True)\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msvm_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl_svm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------------------------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm_results' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "start = time.time()\n",
    "svm_model = svm.SVC(kernel=\"rbf\", gamma = 0.02, C=10)#, max_iter=100000)\n",
    "svm_model.fit(xtrain_transformed_complique,ytrain_transformed_complique)\n",
    "print(\"--- svm fit = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "y_test_pred=svm_model.predict(xtest_transformed_complique)\n",
    "print(\"---SVM y_test_pred = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "\n",
    "\n",
    "cl_svm=classification_report(ytest_transformed_complique,y_test_pred\n",
    "                            ,output_dict=True)\n",
    "svm_results.append(cl_svm)\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04628b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c996e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm start\n",
      "--- svm fit = 7599.587657213211 seconds ---\n",
      "---SVM y_test_pred = 865.5228717327118 seconds ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.91      0.63      0.75      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.96      0.82      0.87    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "---svm = 6004.418404579163 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"svm start\")\n",
    "\n",
    "start_svm = time.time()\n",
    "svm_model = svm.SVC(kernel=\"rbf\", gamma = 0.02, C=10)#, max_iter=100000)\n",
    "svm_model.fit(emb_xtrain_concat,ytrain_transformed_complique)\n",
    "print(\"--- svm fit = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "Y_test_pred_graph=svm_model.predict(emb_xtest_concat)\n",
    "print(\"---SVM y_test_pred = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "print(classification_report(ytest_transformed_complique,Y_test_pred_graph))\n",
    "svm_results.append(classification_report(ytest_transformed_complique,Y_test_pred_graph,output_dict=True))\n",
    "\n",
    "print(\"---svm = %s seconds ---\" % (time.time() - start_svm));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f38f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn recall diff = -0.0242\n",
      "nn f1-score diff = -0.0152\n"
     ]
    }
   ],
   "source": [
    "cl=svm_results[0]\n",
    "cl2=svm_results[1]\n",
    "print(\"nn recall diff = %.4f\" % (cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"]))\n",
    "print(\"nn f1-score diff = %.4f\" % (cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d568b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c64d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python  3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]\n",
      "xgboost version =  1.3.3\n"
     ]
    }
   ],
   "source": [
    "print_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2853a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run clustering techniques on the embeded trash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b921e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff002f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d6530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
