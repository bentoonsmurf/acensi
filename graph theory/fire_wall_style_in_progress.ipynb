{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4de58f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import terminé a : 09:18:07\n"
     ]
    }
   ],
   "source": [
    "#import librairies \n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import progressbar\n",
    "import time\n",
    "from time import process_time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "    \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score,precision_score,recall_score, precision_recall_fscore_support \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn_som.som import SOM\n",
    "import networkx as nx\n",
    "from networkx.algorithms import approximation\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from GEM.gem.utils      import graph_util, plot_util\n",
    "from GEM.gem.evaluation import visualize_embedding as viz\n",
    "from GEM.gem.evaluation import evaluate_graph_reconstruction as gr\n",
    "from GEM.gem.embedding.gf       import GraphFactorization\n",
    "#from GEM.gem.embedding.sdne     import SDNE\n",
    "#from argparse import ArgumentParser\n",
    "#from GraphEmbedding.ge import DeepWalk\n",
    "#from GraphEmbedding.ge import SDNE\n",
    "from karateclub.graph_embedding import Graph2Vec\n",
    "from karateclub.node_embedding.neighbourhood import HOPE\n",
    "from karateclub.node_embedding.neighbourhood import DeepWalk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_frame=200#arbitraire , a tester plus serieusement\n",
    "\n",
    "print(\"import terminé a :\",time.strftime(\"%H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176c2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953c2308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fonctions declaré a : 11:14:24\n"
     ]
    }
   ],
   "source": [
    "#ce dictionaire contien le nom de chaques colones avec la valeur du f1_score\n",
    "#qu'ils on obtenu apres etre ajouté au data_set\n",
    "#initialisé a 0\n",
    "#-1,name1,name2,w,b\n",
    "\n",
    "def convert_dist_to_zero_and_ones(data_set_dist, threshold=0.3):\n",
    "    new_data_set = data_set_dist < threshold\n",
    "    return new_data_set.astype(int)\n",
    "\n",
    "def create_pair_dict(columns_list=[]):\n",
    "    if (columns_list==[]):\n",
    "        print(\"you forgot to input a list of columns\")\n",
    "        return []\n",
    "    pair_dict=dict()\n",
    "    for name1 in columns_list:\n",
    "        for name2 in columns_list:# oui ce n'est pas optimisé\n",
    "            pair_names=name1+\"_\"+name2\n",
    "            reverse_pair_names=name2+\"_\"+name1\n",
    "            if name1!=name2 and pair_names not in pair_dict.keys() and reverse_pair_names not in pair_dict.keys():\n",
    "                pair_dict[pair_names]=[-1,name1,name2,0,0]\n",
    "    \n",
    "    return pair_dict\n",
    "\n",
    "def create_monstruous_pair_data_set_deprecated(pair_dict,panda_data_set):\n",
    "    columns_list = panda_data_set.columns.tolist()\n",
    "    if (columns_list==[]):\n",
    "        print(\"you forgot to input a data_set\")\n",
    "        return {},[]\n",
    "    if (pair_dict=={}):\n",
    "        print(\"you forgot to input a dict\")\n",
    "        return {},[]\n",
    "    #pair_dict=dict()\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(pair_dict));i=0\n",
    "    monstruous_pair_data_set=[]\n",
    "    for pair in pair_dict.items():\n",
    "        pair_dict[pair[0]][0]=i\n",
    "        #print(pair[1][0],pair[1][1],pair[1][2])# value name1,name2\n",
    "        tmp_data_set=panda_data_set[[pair[1][1],pair[1][2]]]\n",
    "        tmp_w,tmp_b=linear_regresion(tmp_data_set)\n",
    "        pair_dict[pair[0]][3]=tmp_w\n",
    "        pair_dict[pair[0]][4]=tmp_b\n",
    "        monstruous_pair_data_set.append(distances(tmp_data_set,tmp_w,tmp_b))\n",
    "        process_bar.update(i);i+=1\n",
    "    #the monstruous data_set should contain 0 and 1\n",
    "    return pair_dict,monstruous_pair_data_set\n",
    "\n",
    "def create_monstruous_pair_data_set(pair_dict,panda_data_set):\n",
    "    columns_list = panda_data_set.columns.tolist()\n",
    "    if (columns_list==[]):\n",
    "        print(\"you forgot to input a data_set\")\n",
    "        return {},[]\n",
    "    if (pair_dict=={}):\n",
    "        print(\"you forgot to input a dict\")\n",
    "        return {},[]\n",
    "    #pair_dict=dict()\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(pair_dict));i=0\n",
    "    monstruous_pair_data_set=np.array([])\n",
    "    for pair in pair_dict.items():\n",
    "        #print(pair[1][0],pair[1][1],pair[1][2])# id,name1,name2,w,b\n",
    "        pair_dict[pair[0]][0]=i\n",
    "        tmp_data_set=panda_data_set[[pair[1][1],pair[1][2]]]\n",
    "        tmp_w,tmp_b=linear_regresion(tmp_data_set)\n",
    "        pair_dict[pair[0]][3]=tmp_w\n",
    "        pair_dict[pair[0]][4]=tmp_b\n",
    "        \n",
    "        if(monstruous_pair_data_set.size ==0):\n",
    "            monstruous_pair_data_set=distances_expand(tmp_data_set,tmp_w,tmp_b)\n",
    "        else:\n",
    "            monstruous_pair_data_set=np.concatenate((monstruous_pair_data_set,distances_expand(tmp_data_set,tmp_w,tmp_b)),axis=1)\n",
    "        process_bar.update(i);i+=1\n",
    "    #the monstruous data_set should contain 0 and 1\n",
    "    return pair_dict,monstruous_pair_data_set\n",
    "\n",
    "def make_one_graph(line ,pair_dict):\n",
    "    g=nx.Graph()\n",
    "    \n",
    "    for pair in pair_dict.items():\n",
    "        #print(pair[1][0],pair[1][1],pair[1][2])# id,name1,name2,w,b\n",
    "        i= pair[1][0]\n",
    "        arc_val=line[i]\n",
    "\n",
    "        if arc_val ==1:\n",
    "            name_1=pair[1][1]\n",
    "            name_2=pair[1][2]\n",
    "            g.add_edge(name_1,name_2)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def create_graphs(monstruous_binary_np_data_set,pair_dict):\n",
    "    start = time.time()\n",
    "    graph_list=[]\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(monstruous_binary_np_data_set));i=0\n",
    "   \n",
    "    for line in monstruous_binary_np_data_set:\n",
    "        #print(type(line),line.shape)\n",
    "\n",
    "        graph_list.append( make_one_graph(line ,pair_dict))\n",
    "        process_bar.update(i);i+=1\n",
    "    print(\"---graph construction = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    " \n",
    "    return graph_list\n",
    "\n",
    "def save_np(np_array,name):\n",
    "    if(\".npy\" not in name):\n",
    "        print(\"you save a numpy array in a file.npy\")\n",
    "        return 0\n",
    "    np.save(name, np_array)\n",
    "\n",
    "def load_np(file_name):\n",
    "    return np.load(file_name)\n",
    "\n",
    "    \n",
    "############################################## liste des options d'extractions\n",
    "def graph_max_degree(g):\n",
    "    degrees = [val for (node, val) in g.degree()]\n",
    "    maxd=max(degrees)\n",
    "    return maxd\n",
    "def geodesic_dist(graph):\n",
    "    return nx.average_shortest_path_length(graph)\n",
    "\n",
    "#special thanks to Francisco A. Rodrigues, University of São Paulo.\n",
    "# http://conteudo.icmc.usp.br/pessoas/francisco\n",
    "def degree_distribution(G):\n",
    "    vk = dict(G.degree())\n",
    "    vk = list(vk.values()) # we get only the degree values\n",
    "    maxk = np.max(vk)\n",
    "    mink = np.min(min)\n",
    "    kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "    Pk = np.zeros(maxk+1) # P(k)\n",
    "    for k in vk:\n",
    "        Pk[k] = Pk[k] + 1\n",
    "    Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "    return kvalues,Pk\n",
    "def shannon_entropy(G):\n",
    "    k,Pk = degree_distribution(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*math.log(p, 2)\n",
    "    return H\n",
    "\n",
    "def contain_meso_scale(graph):\n",
    "    \n",
    "    return False    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_info_from_one_graph(g, to_do_list):\n",
    "    graph_property=np.array([])\n",
    "    calc_result=0\n",
    "    \n",
    "    if(\"max_degree\" in to_do_list):\n",
    "        #print (\" max degree\")\n",
    "        calc_result=graph_max_degree(g)\n",
    "        graph_property=np.append(graph_property,calc_result)\n",
    "    if(\"assortativity\" in to_do_list):\n",
    "        calc_result = nx.degree_assortativity_coefficient(g)\n",
    "        graph_property=np.append(graph_property,calc_result)\n",
    "    if(\"clustering\" in to_do_list):\n",
    "        calc_result= approximation.average_clustering(g, trials=1000, seed=10)\n",
    "        graph_property=np.append(graph_property,calc_result)\n",
    "    if(\"global_efficiency\" in to_do_list):\n",
    "        calc_result= nx.global_efficiency(g)\n",
    "        graph_property=np.append(graph_property,calc_result)   \n",
    "    if(\"geodesic_dist\" in to_do_list):\n",
    "        if nx.is_connected(g):\n",
    "            calc_result=geodesic_dist(g)\n",
    "        else:\n",
    "            calc_result=len(g.nodes()) # = la dist la plus grande\n",
    "        graph_property=np.append(graph_property,calc_result)\n",
    "    if(\"contain_meso_scale\" in to_do_list):#to do\n",
    "        calc_result=contain_meso_scale(g)\n",
    "        graph_property=np.append(graph_property,calc_result)\n",
    "    if(\"Shannon_entropy\" in to_do_list):#to do\n",
    "        calc_result=shannon_entropy(g)\n",
    "        graph_property=np.append(graph_property,calc_result)  \n",
    "    #change this ()\n",
    "    #            ()\n",
    "    #            ()\n",
    "    #      to this ()()()\n",
    "    return np.expand_dims(graph_property,axis=0)\n",
    "\n",
    "def extract_info_from_graphs(graph_list, to_do_list):\n",
    "    \n",
    "    monstruous_info_data_set=np.array([])\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(graph_list));i=0\n",
    "   \n",
    "    \n",
    "    for graph in graph_list:\n",
    "        \n",
    "        if(monstruous_info_data_set.size ==0):\n",
    "            monstruous_info_data_set=extract_info_from_one_graph(graph, to_do_list)\n",
    "        else:\n",
    "            monstruous_info_data_set=np.concatenate((monstruous_info_data_set,extract_info_from_one_graph(graph, to_do_list)),axis=0)\n",
    "        process_bar.update(i);i+=1\n",
    "    \n",
    "    return monstruous_info_data_set\n",
    "\n",
    "\n",
    "\n",
    "def create_graphs_to_delete(monstruous_binary_np_data_set,pair_dict):\n",
    "    start = time.time()\n",
    "    graph_list=[]\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(monstruous_binary_np_data_set));i=0\n",
    "   \n",
    "    for line in monstruous_binary_np_data_set:\n",
    "        #print(type(line),line.shape)\n",
    "\n",
    "        graph_list.append( make_one_graph(line ,pair_dict))\n",
    "        process_bar.update(i);i+=1\n",
    "    print(\"---graph construction = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_info_from_binary_data_set(monstruous_binary_np_data_set,pair_dict, to_do_list):\n",
    "    start_time = process_time() \n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(monstruous_binary_np_data_set));i=0\n",
    "    monstruous_info_data_set=np.array([])\n",
    "    counter = 0\n",
    "    contruct_time=0\n",
    "    calcul_time=0\n",
    "    for line in monstruous_binary_np_data_set:\n",
    "        in_loop_time = process_time()\n",
    "        graph=make_one_graph(line ,pair_dict)\n",
    "        contruct_time = contruct_time+ process_time()  - in_loop_time\n",
    "        \n",
    "        in_loop_time = process_time()\n",
    "        if(monstruous_info_data_set.size ==0):\n",
    "            monstruous_info_data_set=extract_info_from_one_graph(graph, to_do_list)\n",
    "        else:\n",
    "            monstruous_info_data_set=np.concatenate((monstruous_info_data_set,extract_info_from_one_graph(graph, to_do_list)),axis=0)\n",
    "        calcul_time = calcul_time+ process_time()  - in_loop_time\n",
    "        process_bar.update(i);i+=1\n",
    "        \n",
    "        #####################################################################\n",
    "        del graph # je delete le graph pour etre sur de liberer la memoire\n",
    "        #gc.collect() #je n'appelle pas le garbageColector = cout 22 heures\n",
    "        #calcul_time =  521.984375 contruct_time 59.3125\n",
    "        #counter=counter+1\n",
    "        #if(counter == 100000):  \n",
    "            #print(process_time()  - start_time, \"seconds\");start_time = process_time()\n",
    "            #print(\"calcul_time = \",calcul_time,\"contruct_time\",contruct_time)\n",
    "            #return monstruous_info_data_set\n",
    "            \n",
    "        \n",
    "    \n",
    "    print(\"---graph extraction = %s seconds ---\" % (process_time()  - start_time));start_time = process_time()\n",
    "    return monstruous_info_data_set\n",
    "\n",
    "# Function to find distance line equation ax + by + c =0\n",
    "#y=mx+c  ----> mx -y +c =0  ---> b=-1 ,a=weight ,c=bias\n",
    "def shortest_distance(x1, y1, a, b, c):\n",
    "    \n",
    "    d = abs((a * x1 + b * y1 + c)) / (math.sqrt(a * a + b * b))\n",
    "    #print(\"Perpendicular distance is \",d)\n",
    "    return d\n",
    "\n",
    "def distances(data_set,w,b):\n",
    "    np_data=data_set.to_numpy()\n",
    "    np_dist=[]\n",
    "    for line in np_data:\n",
    "        x1=line[0]\n",
    "        y1=line[1]\n",
    "        np_dist.append(shortest_distance(x1, y1, w, -1, b))\n",
    "    \n",
    "    return np.array(np_dist)\n",
    "\n",
    "def distances_expand(data_set,w,b):\n",
    "    np_data=data_set.to_numpy()\n",
    "    np_dist=[]\n",
    "    for line in np_data:\n",
    "        x1=line[0]\n",
    "        y1=line[1]\n",
    "        np_dist.append(shortest_distance(x1, y1, w, -1, b))\n",
    "    \n",
    "    return np.expand_dims( np.array(np_dist)  ,axis=1)\n",
    "# y = mx +b\n",
    "def line_equation_from_two_points(x1,y1,x2,y2):\n",
    "    m=(y2-y1)/(x2-x1)\n",
    "    b=y1-m*x1\n",
    "    return m,b\n",
    "\n",
    "\n",
    "\n",
    "def shortest_distance_test():\n",
    "    print(shortest_distance(0, 0, 1, -1, 1))\n",
    "    print(shortest_distance(-1, 0, 1, -1, 1))\n",
    "    print(shortest_distance(1, 0, 1, -1, 1))\n",
    "    print(math.sqrt(2))\n",
    "    print(shortest_distance(3, 0, 1, -1, 1))\n",
    "    print(math.sqrt(2*2+2*2))\n",
    "\n",
    "\n",
    "#https://www.machinelearningplus.com/deep-learning/linear-regression-tensorflow/\n",
    "###############################################################################\n",
    "################# partie linear regression ####################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def lin_reg_bootstrap(panda_data_set,nb_iter=1000):\n",
    "    columns_list = panda_data_set.columns.tolist()\n",
    "    x_train=NormalizeData(panda_data_set[columns_list[0]].to_numpy())\n",
    "    y_train=NormalizeData(panda_data_set[columns_list[1]].to_numpy())\n",
    "    data_set_size=len(x_train)\n",
    "    m=0\n",
    "    b=0\n",
    "    counter=0\n",
    "    for i in range(nb_iter):\n",
    "        p1=random.randint(0, data_set_size-1)\n",
    "        p2=random.randint(0, data_set_size-1)\n",
    "        \n",
    "        x1=x_train[p1]\n",
    "        y1=y_train[p1]        \n",
    "        x2=x_train[p2]\n",
    "        y2=y_train[p2]\n",
    "        if((x2-x1)!=0):\n",
    "            m_tmp,b_tmp=line_equation_from_two_points(x1,y1,x2,y2)\n",
    "            m=m+m_tmp\n",
    "            b=b+b_tmp\n",
    "        else:\n",
    "            counter=counter+1\n",
    "    m=m/(nb_iter-counter)\n",
    "    b=b/(nb_iter-counter)\n",
    "    return m,b\n",
    "    \n",
    "def linreg(x,weight,bias):#x is a np list\n",
    "    y = weight*x + bias\n",
    "    return y\n",
    "\n",
    "# Define loss function (MSE)\n",
    "def squared_error(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "def not_squared_error(y_pred, y_true):\n",
    "    return tf.reduce_mean(y_pred - y_true)\n",
    "\n",
    "def linear_regresion(panda_data_set,boot_iter=1000):\n",
    "    columns_list = panda_data_set.columns.tolist()\n",
    "    #x_train=panda_data_set[columns_list[0]].to_numpy()\n",
    "    #y_train=panda_data_set[columns_list[1]].to_numpy()\n",
    "    x_train=NormalizeData(panda_data_set[columns_list[0]].to_numpy())\n",
    "    y_train=NormalizeData(panda_data_set[columns_list[1]].to_numpy())\n",
    "    #print(\"x_train max =\",np.amax(x_train, axis=0))\n",
    "    #print(\"y_train max =\",np.amax(y_train, axis=0))\n",
    "    learning_rate = 0.01\n",
    "    # Number of loops for training through all your data to update the parameters\n",
    "    training_epochs = 100\n",
    "    \n",
    "    boot_weight,boot_bias=lin_reg_bootstrap(panda_data_set,nb_iter=boot_iter)\n",
    "    weight = tf.Variable(boot_weight)\n",
    "    bias   = tf.Variable(boot_bias)\n",
    "    #weight = tf.Variable(0.)\n",
    "    #bias   = tf.Variable(0.)\n",
    "    \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Compute loss within Gradient Tape context\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_predicted = linreg(x_train,weight,bias)\n",
    "            #loss = squared_error(y_predicted, y_train)\n",
    "            loss = not_squared_error(y_predicted, y_train)\n",
    "            # Get gradients\n",
    "            gradients = tape.gradient(loss, [weight,bias])\n",
    "\n",
    "            # Adjust weights\n",
    "            weight.assign_sub(gradients[0]*learning_rate)\n",
    "            bias.assign_sub(gradients[1]*learning_rate)\n",
    "\n",
    "    \n",
    "    \n",
    "    return weight.numpy(),bias.numpy()\n",
    "\n",
    "print(\"fonctions declaré a :\",time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9e678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67ec17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'merchant', 'category', 'amt', 'gender', 'state', 'zip', 'lat', 'long', 'city_pop', 'dob', 'unix_time', 'merch_lat', 'merch_long', 'delta_time', 'delta_amt', 'delta_time_category', 'delta_amt_category', 'delta_time_merchant', 'delta_amt_merchant', 'avg_amt', 'delta_avg_amt', 'avg_amt_category', 'delta_avg_amt_category', 'avg_amt_merchant', 'avg_amt_state', 'avg_amt_city', 'avg_amt_job', 'delta_avg_amt_category_job', 'month', 'day', 'hour']\n",
      "--- import data_set = 20.039904832839966 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#import le data_set complet --- import data_set = 28.00195622444153 seconds ---\n",
    "start = time.time()\n",
    "\n",
    "data_file= os.path.abspath('../../data')\n",
    "full_path=data_file+'\\\\'+'fraudTrain.csv'   # unmodified train set\n",
    "train_df=pd.read_csv(full_path)\n",
    "full_path=data_file+'\\\\'+'fraudTest.csv'\n",
    "test_df=pd.read_csv(full_path)\n",
    "\n",
    "\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_train_1_2_svm.csv'\n",
    "xtrain_transformed_complique=pd.read_csv(full_path)\n",
    "ytrain_transformed_complique=train_df['is_fraud'].iloc[:int(len(train_df)*0.8)]\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_val_1_2_svm.csv'\n",
    "xval_transformed_complique=pd.read_csv(full_path)\n",
    "yval_transformed_complique=train_df['is_fraud'].iloc[len(ytrain_transformed_complique):]\n",
    "\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_test_1_2_svm.csv'\n",
    "xtest_transformed_complique=pd.read_csv(full_path)\n",
    "ytest_transformed_complique=test_df['is_fraud']\n",
    "\n",
    "\n",
    "cols = xtrain_transformed_complique.columns.tolist()\n",
    "print(cols)\n",
    "print(\"--- import data_set = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9f590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n",
      "1024\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "dict_378 = create_pair_dict(cols)\n",
    "print (len(dict_378))\n",
    "print(32*32)\n",
    "print(len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67b517c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10740133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (495 of 496) |##################### | Elapsed Time: 0:24:41 ETA:   0:00:04"
     ]
    }
   ],
   "source": [
    "#create data_set  ---create_monstruous_pair_data_set = ?????.00195622444153 seconds ---\n",
    "start = time.time()\n",
    "train_dict,train_graph_dist = create_monstruous_pair_data_set(dict_378,xtrain_transformed_complique)\n",
    "val_dict,val_graph_dist = create_monstruous_pair_data_set(dict_378,xval_transformed_complique)\n",
    "test_dict,test_graph_dist = create_monstruous_pair_data_set(dict_378,xtest_transformed_complique)\n",
    "print(\"--- create_graph_dist = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30fbf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_binary=convert_dist_to_zero_and_ones(train_graph_dist)\n",
    "val_graph_binary=convert_dist_to_zero_and_ones(val_graph_dist)\n",
    "test_graph_binary=convert_dist_to_zero_and_ones(test_graph_dist)\n",
    "print(\"--- convert_dist_to_zero_and_ones = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dfd41fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (21 of 259335) |                    | Elapsed Time: 0:00:00 ETA:   0:21:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---graph extraction = 14324.84375 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (20 of 555719) |                    | Elapsed Time: 0:00:00 ETA:  00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---graph extraction = 1953.75 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99% (555717 of 555719) |############### | Elapsed Time: 1:32:15 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---graph extraction = 5520.125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#crash a 22% pour cause de memoire ?\n",
    "#graph_list=create_graphs(M_le_binary,dict_378_binary)\n",
    "\n",
    "# 1000 iter -->90 sec\n",
    "#to_do_list=[\"max_degree\",\"assortativity\",\"Clustering\"]\n",
    "\n",
    "# 1000 iter --> 91  sec\n",
    "to_do_list=[\"max_degree\",\"assortativity\",\"Clustering\",\"global_efficiency\",\"geodesic_dist\",\"Shannon_entropy\"]\n",
    "\n",
    "#2h 7%\n",
    "train_graph_properties=extract_info_from_binary_data_set(train_graph_binary,train_dict,to_do_list)\n",
    "val_graph_properties=extract_info_from_binary_data_set(val_graph_binary,val_dict,to_do_list)\n",
    "test_graph_properties=extract_info_from_binary_data_set(test_graph_binary,test_dict,to_do_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9274a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037340, 32)\n",
      "(1037340, 5)\n",
      "(1037340, 37)\n"
     ]
    }
   ],
   "source": [
    "## ajouter des colones\n",
    "\n",
    "\n",
    "print(xtrain_transformed_complique.shape)\n",
    "print(train_graph_properties.shape)\n",
    "\n",
    "X_train_plus_graph =np.concatenate((xtrain_transformed_complique,train_graph_properties),axis=1)\n",
    "print(X_train_plus_graph.shape)\n",
    "\n",
    "X_val_plus_graph =np.concatenate((xval_transformed_complique,val_graph_properties),axis=1)\n",
    "X_test_plus_graph =np.concatenate((xtest_transformed_complique,test_graph_properties),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177000d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af4d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "387153d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin.marty\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:23:16] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       0.98      0.83      0.90      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       0.99      0.91      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.95      0.79      0.86      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.98      0.89      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=50, gamma=0.05,eta=0.05,max_depth=7, n_jobs=16)\n",
    "\n",
    "xgb.fit(xtrain_transformed_complique,ytrain_transformed_complique)\n",
    "Y_train_pred=xgb.predict(xtrain_transformed_complique)\n",
    "Y_test_pred=xgb.predict(xtest_transformed_complique)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred))\n",
    "print(classification_report(ytest_transformed_complique,Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "283cc787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjamin.marty\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:25:00] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       0.98      0.83      0.90      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       0.99      0.91      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.94      0.79      0.86      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.97      0.89      0.93    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "---XGboost avec modification = 29314.38864517212 seconds ---\n"
     ]
    }
   ],
   "source": [
    "X_train_plus_graph\n",
    "xgb.fit(X_train_plus_graph,ytrain_transformed_complique)\n",
    "Y_train_pred_graph=xgb.predict(X_train_plus_graph)\n",
    "Y_test_pred_graph=xgb.predict(X_test_plus_graph)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred_graph))\n",
    "print(classification_report(ytest_transformed_complique,Y_test_pred_graph))\n",
    "\n",
    "print(\"---XGboost avec modification = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f38d127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost recall diff = -0.0014\n",
      "xgboost f1-score diff = -0.0035\n"
     ]
    }
   ],
   "source": [
    "cl=classification_report(ytest_transformed_complique,Y_test_pred,output_dict=True)\n",
    "cl2=classification_report(ytest_transformed_complique,Y_test_pred_graph,output_dict=True)\n",
    "recall_diff=(cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"])\n",
    "f1_diff=(cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "print(\"xgboost recall diff = %.4f\" % recall_diff)\n",
    "print(\"xgboost f1-score diff = %.4f\" % f1_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30eed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0daa2a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:   49.6s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:  4.3min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForest sans ajout = 263.48823714256287 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:  4.9min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RandomForest avec embedding = 297.68414855003357 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 140 out of 140 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "max_depth = 15\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=max_depth, min_samples_leaf= 3, min_samples_split= 2, n_estimators= 140, verbose=1, n_jobs=16)\n",
    "rf.fit(xtrain_transformed_complique, ytrain_transformed_complique)\n",
    "# Prediction for the training/validation set\n",
    "y_train_pred = rf.predict(xtrain_transformed_complique)\n",
    "y_test_pred = rf.predict(xtest_transformed_complique)\n",
    "\n",
    "print(\"---RandomForest sans ajout = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "max_depth = 15\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=max_depth, min_samples_leaf= 3, min_samples_split= 2, n_estimators= 140, verbose=1, n_jobs=16)\n",
    "rf.fit(X_train_plus_graph, ytrain_transformed_complique)\n",
    "# Prediction for the training/validation set\n",
    "y_train_pred_graph = rf.predict(X_train_plus_graph)\n",
    "y_test_pred_graph= rf.predict(X_test_plus_graph)\n",
    "\n",
    "\n",
    "print(\"---RandomForest avec graph = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ce98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       1.00      0.83      0.90      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       1.00      0.91      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.97      0.73      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.99      0.86      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1031372\n",
      "           1       1.00      0.83      0.91      5968\n",
      "\n",
      "    accuracy                           1.00   1037340\n",
      "   macro avg       1.00      0.92      0.95   1037340\n",
      "weighted avg       1.00      1.00      1.00   1037340\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.97      0.73      0.83      2145\n",
      "\n",
      "    accuracy                           1.00    555719\n",
      "   macro avg       0.98      0.86      0.92    555719\n",
      "weighted avg       1.00      1.00      1.00    555719\n",
      "\n",
      "xgboost recall diff = -0.0014\n",
      "xgboost f1-score diff = -0.0035\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytrain_transformed_complique,y_train_pred))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(classification_report(ytrain_transformed_complique,y_train_pred_graph))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred_graph))\n",
    "\n",
    "cl=classification_report(ytest_transformed_complique,Y_test_pred,output_dict=True)\n",
    "cl2=classification_report(ytest_transformed_complique,Y_test_pred_graph,output_dict=True)\n",
    "recall_diff=(cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"])\n",
    "f1_diff=(cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "print(\"rf recall diff = %.4f\" % recall_diff)\n",
    "print(\"rf f1-score diff = %.4f\" % f1_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2791ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4168d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8105/8105 [==============================] - 17s 2ms/step - loss: 0.0382 - accuracy: 0.9938 - val_loss: 0.0370 - val_accuracy: 0.9941\n",
      "Epoch 2/100\n",
      "8105/8105 [==============================] - 16s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0364 - val_accuracy: 0.9941\n",
      "Epoch 3/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0365 - val_accuracy: 0.9941\n",
      "Epoch 4/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0365 - val_accuracy: 0.9941\n",
      "Epoch 5/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0366 - val_accuracy: 0.9941\n",
      "Epoch 6/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0367 - val_accuracy: 0.9941\n",
      "Epoch 7/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0363 - val_accuracy: 0.9941\n",
      "Epoch 8/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0354 - accuracy: 0.9942 - val_loss: 0.0363 - val_accuracy: 0.9941\n",
      "Epoch 9/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0354 - accuracy: 0.9942 - val_loss: 0.0364 - val_accuracy: 0.9941\n",
      "Epoch 10/100\n",
      "8105/8105 [==============================] - 14s 2ms/step - loss: 0.0354 - accuracy: 0.9942 - val_loss: 0.0364 - val_accuracy: 0.9941\n",
      "Epoch 11/100\n",
      "8105/8105 [==============================] - 14s 2ms/step - loss: 0.0354 - accuracy: 0.9942 - val_loss: 0.0363 - val_accuracy: 0.9941\n",
      "Epoch 12/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0365 - val_accuracy: 0.9941\n",
      "Epoch 13/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0365 - val_accuracy: 0.9941\n",
      "Epoch 14/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0366 - val_accuracy: 0.9941\n",
      "Epoch 15/100\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0354 - accuracy: 0.9942 - val_loss: 0.0365 - val_accuracy: 0.9941\n",
      "Epoch 16/100\n",
      "8076/8105 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9943Restoring model weights from the end of the best epoch: 1.\n",
      "8105/8105 [==============================] - 13s 2ms/step - loss: 0.0355 - accuracy: 0.9942 - val_loss: 0.0364 - val_accuracy: 0.9941\n",
      "Epoch 00016: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(len(xtrain_transformed_complique.columns),), activation='relu')),\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu')),\n",
    "model.add(Dense(8, activation='relu')),\n",
    "model.add(Dense(4, activation='relu')),\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) #metrics\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=15, verbose=1,mode='auto', baseline=None, restore_best_weights=True)\n",
    "history= model.fit(xtrain_transformed_complique.values, ytrain_transformed_complique.values, epochs = 100, batch_size=128, verbose = 1, validation_data=(xval_transformed_complique, yval_transformed_complique), callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9414d718",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BENJAM~1.MAR\\AppData\\Local\\Temp/ipykernel_3964/2103745853.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_plus_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "print(len(X_train_plus_graph.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b4d449b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hope_xtrain_concat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BENJAM~1.MAR\\AppData\\Local\\Temp/ipykernel_3964/3668715283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m############## a modifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhope_xtrain_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hope_xtrain_concat' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "############## a modifier\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(32, input_shape=(len(X_train_plus_graph.columns),), activation='relu')),\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(16, activation='relu')),\n",
    "model2.add(Dense(8, activation='relu')),\n",
    "model2.add(Dense(4, activation='relu')),\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n",
    "model2.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) #metrics\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=15, verbose=1,mode='auto', baseline=None, restore_best_weights=True)\n",
    "history= model2.fit(X_train_plus_graph.values, hope_ytrain_concat.values, epochs = 100, batch_size=128, verbose = 1, validation_data=(hope_xval_concat, yval_transformed_complique), callbacks = [earlystopper])\n",
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train_pred =(model.predict(xtrain_transformed_complique) > 0.5).astype(\"int32\")\n",
    "Y_test_pred =(model.predict(xtest_transformed_complique) > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "y_train_pred_graph =(model2.predict(hope_xtrain_concat) > 0.5).astype(\"int32\")\n",
    "y_test_pred_graph =(model2.predict(hope_xtest_concat) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a30f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NN sans modifications\")\n",
    "print(classification_report(ytrain_transformed_complique,y_train_pred))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print(\"NN avec des info de graph en plus\")\n",
    "print(classification_report(ytrain_transformed_complique,y_train_pred_graph))\n",
    "print(classification_report(ytest_transformed_complique,y_test_pred_graph))\n",
    "\n",
    "cl=classification_report(ytest_transformed_complique,Y_test_pred,output_dict=True)\n",
    "cl2=classification_report(ytest_transformed_complique,Y_test_pred_graph,output_dict=True)\n",
    "recall_diff=(cl2[\"1\"][\"recall\"]-cl[\"1\"][\"recall\"])\n",
    "f1_diff=(cl2[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "print(\"NN recall diff = %.4f\" % recall_diff)\n",
    "print(\"NN f1-score diff = %.4f\" % f1_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b378ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19d4de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1dbce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_too_long=False\n",
    "if(not_too_long):#72h d'execution \n",
    "    from sklearn import svm\n",
    "    start = time.time()\n",
    "    svm_model = svm.SVC(kernel=\"rbf\", gamma = 0.02, C=10)\n",
    "    svm_model.fit(xtrain_transformed_complique,ytrain_transformed_complique)\n",
    "    Y_train_pred=xgb.predict(xtrain_transformed_complique)\n",
    "    Y_test_pred=xgb.predict(xtest_transformed_complique)\n",
    "    print(\"---SVM sans modification = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    print(classification_report(ytrain_transformed_complique,y_train_pred))\n",
    "    print(classification_report(ytest_transformed_complique,y_test_pred))\n",
    "    cl=classification_report(ytest_transformed_complique,Y_test_pred,output_dict=True))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    ####\n",
    "    svm_model.fit(X_train_plus_graph,ytrain_transformed_complique)\n",
    "    y_train_pred_graph=xgb.predict(X_train_plus_graph)\n",
    "    y_test_pred_graph=xgb.predict(X_test_plus_graph)\n",
    "    print(\"---SVM avec plus d informations = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    print(classification_report(ytrain_transformed_complique,y_train_pred_graph))\n",
    "    print(classification_report(ytest_transformed_complique,y_test_pred_graph))\n",
    "    cl2=classification_report(ytest_transformed_complique,y_test_pred_graph,output_dict=True)\n",
    "    print(\"recall diff =\",cl[\"1\"][\"recall\"]-cl2[\"1\"][\"recall\"])\n",
    "    print(\"f1-score diff =\",cl[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bcb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b46ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bab91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dict_378)\n",
    "print((M_le_maudit[0].shape),\"x\",len(M_le_maudit),\"[\",M_le_maudit[0][0],\"]\")\n",
    "print(type(M_le_maudit[0]),\"x\",type(M_le_maudit),\"[\",type(M_le_maudit[0][0]),\"]\")\n",
    "\n",
    "print((M_le_maudit_binary.shape),\"[\",M_le_maudit_binary[0][0:10],\"]\")\n",
    "print(type(M_le_maudit_binary[0]),\"x\",type(M_le_maudit_binary),\"[\",type(M_le_maudit_binary[0][0]),\"]\")\n",
    "\n",
    "\n",
    "print((M_le_binary.shape),\"[\",M_le_binary[0][0:10],\"]\")\n",
    "print(type(M_le_binary[0]),\"x\",type(M_le_binary),\"[\",type(M_le_binary[0][0]),\"]\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3998129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ac1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(xtrain_transformed_complique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef968321",
   "metadata": {},
   "outputs": [],
   "source": [
    "panda_test=xtrain_transformed_complique.copy()[[\"delta_amt\"]]\n",
    "print((panda_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcf1b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = dict_378[\"amt_hour\"]\n",
    "print((pair))\n",
    "tmp_data_set=xtrain_transformed_complique.copy()[[pair[1],pair[2]]]\n",
    "print((tmp_data_set).shape)\n",
    "columns_list = tmp_data_set.columns.tolist()\n",
    "print(columns_list)\n",
    "x_train_not_normalized=(tmp_data_set[columns_list[0]].to_numpy())\n",
    "y_train_not_normalized=(tmp_data_set[columns_list[1]].to_numpy())\n",
    "\n",
    "x_train=NormalizeData(tmp_data_set[columns_list[0]].to_numpy())\n",
    "y_train=NormalizeData(tmp_data_set[columns_list[1]].to_numpy())\n",
    "print(len(tmp_data_set))\n",
    "\n",
    "print(type(x_train))\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c50ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.show()\n",
    "plt.scatter(x_train_not_normalized, y_train_not_normalized)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e66a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b =linear_regresion(tmp_data_set,boot_iter=10000)\n",
    "boot_w,boot_b =lin_reg_bootstrap(tmp_data_set,nb_iter=10000)\n",
    "np_dist=distances(tmp_data_set,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede57fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_dist.shape)\n",
    "exp_np_dist=np.expand_dims(   ,axis=1)\n",
    "print(exp_np_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca474af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w= \",(w))\n",
    "print(\"b= \",b)\n",
    "print(\"boot_w= \",(boot_w))\n",
    "print(\"boot_b= \",boot_b)\n",
    "plt.ylim(-0.30, 1.3)  \n",
    "plt.scatter(x_train,y_train)\n",
    "plt.plot(x_train,x_train*w +b,\"r\")\n",
    "plt.plot(x_train,x_train*boot_w +boot_b,\"g\")\n",
    "\n",
    "plt.show()\n",
    "#w = -406.7335090369642\n",
    "#b = -0.44518610972447087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e1040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_test=xtest_transformed_complique.to_numpy()\n",
    "normalized_np_test=NormalizeData(np_test)\n",
    "print(np_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8446168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np_test[0])\n",
    "print(normalized_np_test[0])\n",
    "count=0\n",
    "for obj in normalized_np_test:\n",
    "    if count == 0:\n",
    "        print (obj)\n",
    "    count=count+1\n",
    "print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajouter  une nouvelle colonne\n",
    "'''\n",
    "print(X_train_transformed.shape)\n",
    "print(np.shape(moy))\n",
    "moy=np.expand_dims(moy, axis=1)\n",
    "print(moy.shape)\n",
    "X_train_transformed =np.concatenate((X_train_transformed,moy),axis=1)\n",
    "print(X_train_transformed.shape)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([])\n",
    "if(a.size ==0):\n",
    "    a=np.array(list([0,0,0]))\n",
    "b=np.array(list([1,1,1]))\n",
    "print(np.shape(a))\n",
    "a_=np.expand_dims(a,axis=1)\n",
    "b_=np.expand_dims(b,axis=1)\n",
    "print(np.shape(a_))\n",
    "aray_test=np.concatenate((a,b),axis=0)\n",
    "\n",
    "aray_test_=np.concatenate((a_,b_),axis=1)\n",
    "print(aray_test_)\n",
    "aray_test_=np.concatenate((aray_test_,b_*2),axis=1)\n",
    "#aray_test=(a+b)\n",
    "\n",
    "print(aray_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_aray_test_=aray_test_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a6fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list=list(dict_378.keys())\n",
    "for line in nd_aray_test_:\n",
    "    #print(type(line),line.shape)\n",
    "    print(line)\n",
    "    for i in np.arange(len(line)):\n",
    "        print (line[i])\n",
    "        print (key_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072429b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_378_binary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f537f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_378_binary[\"category_day\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_378_binary.items()\n",
    "\n",
    "if(True):\n",
    "    for pair in dict_378_binary.items():\n",
    "        pair_dict[pair[0]][3]\n",
    "        pair_dict[pair[0]][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631bc71d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
