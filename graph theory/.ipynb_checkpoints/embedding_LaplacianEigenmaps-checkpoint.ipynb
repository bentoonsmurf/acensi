{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090123ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import terminé a : 09:25:30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importation des librairies \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import progressbar\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score,precision_score,recall_score, precision_recall_fscore_support \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn_som.som import SOM\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from GEM.gem.utils      import graph_util, plot_util\n",
    "from GEM.gem.evaluation import visualize_embedding as viz\n",
    "from GEM.gem.evaluation import evaluate_graph_reconstruction as gr\n",
    "from GEM.gem.embedding.gf       import GraphFactorization\n",
    "#from GEM.gem.embedding.sdne     import SDNE\n",
    "#from argparse import ArgumentParser\n",
    "#from GraphEmbedding.ge import DeepWalk\n",
    "#from GraphEmbedding.ge import SDNE\n",
    "from karateclub.graph_embedding import Graph2Vec\n",
    "from karateclub.node_embedding.neighbourhood import HOPE\n",
    "from karateclub.node_embedding.neighbourhood import DeepWalk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_frame=200#arbitraire , a tester plus serieusement\n",
    "\n",
    "print(\"import terminé a :\",time.strftime(\"%H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03fb8668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fonctions declaré a : 09:25:30\n"
     ]
    }
   ],
   "source": [
    "def instals():\n",
    "    !pip install progressbar2\n",
    "    !pip3 install PyQt5\n",
    "    \n",
    "    \n",
    "\n",
    "def create_graph(data_set):\n",
    "    g = nx.MultiGraph()\n",
    "\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while (i<len(data_set)):\n",
    "        a=data_set[\"merchant\"][i]\n",
    "        b=data_set[\"cc_num\"][i]\n",
    "        g.add_edge(a,b,weight=1,edge_id=i)\n",
    "        i=i+1\n",
    "    \n",
    "    print(\"---graph construction = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return g\n",
    "\n",
    "\n",
    "def fill(g,liste):\n",
    "  \n",
    "    while(liste):\n",
    "        a=liste.pop()\n",
    "        i=0\n",
    "        while(i<len(liste)):\n",
    "            b=liste[i]\n",
    "            g.add_edge(a,b,weight=1)\n",
    "            i+=1\n",
    "    \n",
    "    #return g\n",
    "\n",
    "def init_sub_graph(nb_frames):\n",
    "    # divison en plusieures sous graphs \n",
    "    sous_graph=[]\n",
    "    i=0\n",
    "    i2=0\n",
    "    sub_g=0\n",
    "    while ( i<nb_frames):\n",
    "        sous_graph.append(nx.MultiGraph())\n",
    "        i=i+1\n",
    "    return sous_graph\n",
    "\n",
    "def bipartite_dict(dict_merchants,dict_cc_num):\n",
    "\n",
    "    dict_merchants_copy=dict_merchants.copy()\n",
    "    dict_merchants_copy = dict([(value, key) for key, value in dict_merchants_copy.items()])\n",
    "    dict_cc_num_copy=dict_cc_num.copy()\n",
    "    dict_cc_num_copy = dict([(value, key) for key, value in dict_cc_num_copy.items()])\n",
    "\n",
    "    for key in dict_merchants_copy.keys():\n",
    "        dict_merchants_copy[key] = 0\n",
    "    for key in dict_cc_num_copy.keys():\n",
    "        dict_cc_num_copy[key] = 1\n",
    "    return dict_merchants_copy,dict_cc_num_copy\n",
    "\n",
    "def create_sub_graph(data_set,nb_frames,dict_merchants,dict_cc_num):\n",
    "    sub_g=init_sub_graph(nb_frames)\n",
    "    print (len(sub_g),\" sous graphs\")\n",
    "    #sg in sub_g\n",
    "    dict_merchants_copy,dict_cc_num_copy=bipartite_dict(dict_merchants,dict_cc_num)    \n",
    "    time_frame_size=len(data_set) / nb_frames\n",
    "    start = time.time()\n",
    "    connected_count=0\n",
    "    i=0\n",
    "    i2=0\n",
    "    sg=0\n",
    "    while (i<len(data_set)):\n",
    "        a=data_set[\"merchant\"][i]# rendre plus lisible\n",
    "        b=data_set[\"cc_num\"][i]\n",
    "        sub_g[sg].add_edge(a,b,weight=1,edge_id=i,edge_rid=i2)\n",
    "\n",
    "        i=i+1\n",
    "        i2=i2+1\n",
    "        if i2>= time_frame_size:\n",
    "            nx.set_node_attributes(sub_g[sg], dict_merchants_copy, \"bipartite\")\n",
    "            nx.set_node_attributes(sub_g[sg], dict_cc_num_copy, \"bipartite\")\n",
    "            i2=0\n",
    "            if(nx.is_connected(sub_g[sg])):\n",
    "                connected_count+=1\n",
    "            sg=sg+1\n",
    "\n",
    "    print(\"---graph split = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "    return sub_g,connected_count\n",
    "\n",
    "\n",
    "def invert_graph(gr):\n",
    "    new_graph=nx.MultiGraph()\n",
    "    #tous les arc d'un sommet sont connecté entre eux\n",
    "    #step 1 = dans new_graph creer un sommet pour chaque arc\n",
    "\n",
    "    for edge in gr.edges(data=True):\n",
    "        #print(edge)\n",
    "        node_id=edge[2][\"edge_rid\"]\n",
    "        _edge_id=edge[2][\"edge_id\"]\n",
    "        new_graph.add_node(node_id,edge_id=_edge_id)\n",
    "\n",
    "    #pour chaque node de gr relier enssemble tous les arc\n",
    "    for node in gr.nodes:\n",
    "        _edge_list=list(gr.edges(node,data=\"edge_rid\"))\n",
    "        node_list=[]\n",
    "        i=0\n",
    "        #print(_edge_list)\n",
    "        for items in _edge_list:\n",
    "            #print(node[0])\n",
    "            node_list.append(_edge_list[i][2])\n",
    "            i=i+1            \n",
    "        fill(new_graph,node_list)\n",
    "\n",
    "\n",
    "    return new_graph\n",
    "\n",
    "\n",
    "def create_inverted_sub_graph(sub_g):\n",
    "    \n",
    "    \n",
    "    process_bar = progressbar.ProgressBar().start(max_value=len(sub_g))\n",
    "\n",
    "    i=0\n",
    "    start = time.time\n",
    "    inv_sous_graph=[]\n",
    "    for sg in sub_g:\n",
    "        inv_sous_graph.append(invert_graph(sg))\n",
    "        i=i+1\n",
    "        process_bar.update(i)\n",
    "        \n",
    "    return inv_sous_graph\n",
    "    print(\"---create_inverted_sub_graph  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "\n",
    "\n",
    "\n",
    "def start_time_eval():\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while(i<1000000):\n",
    "        i=i+1\n",
    "    boucle_time=time.time() - start\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while(i<1000000):\n",
    "        poubelle =time.time()\n",
    "        i=i+1\n",
    "    print(\"---1 milion de time.time=  %s seconds ---\" % (time.time() - start-boucle_time));start = time.time()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#return le nombre d'arete du graph weighted_g\n",
    "def nb_edge(weighted_g):\n",
    "    summ=0\n",
    "    NODES = list(weighted_g.nodes)\n",
    "    for node in NODES:\n",
    "        summ= summ+G.degree[node]\n",
    "    return summ/2\n",
    "\n",
    "def replissement(weighted_g, nb_merc,nb_cc_num):\n",
    "    nb_edges=nb_edge(weighted_g)\n",
    "    nb_max_edges =(nb_merc* nb_cc_num)#graph bipati\n",
    "    return  nb_edges / nb_max_edges\n",
    "    \n",
    "def edge_repartition(g,len_dict_merchants):\n",
    "    repartition=[]\n",
    "    #remplissage de repartiotion avec 0 pour eviter les bugg\n",
    "    \n",
    "    #for each vertex \n",
    "        # for each edge in vertex.edges\n",
    "            #repartition[ edge.poid ] ++\n",
    "    print (\"\")\n",
    "    \n",
    "\n",
    "def slow_concat(d1,d2):\n",
    "    return dict(d1.items() | d2.items())\n",
    "\n",
    "def ditc_maping_so_slow_but_why(X_train_ultra_simple,dict_merchants,dict_cc_num):\n",
    "    #---dictionary maping = 4272.313026428223 seconds ---\n",
    "    start = time.time()\n",
    "    size =len(X_train_ultra_simple)\n",
    "    i=0\n",
    "    time_val=[]\n",
    "    while (i<size):\n",
    "        X_train_ultra_simple.iat[i,0]=dict_merchants[X_train_ultra_simple.iat[i,0]]\n",
    "        X_train_ultra_simple.iat[i,1]=dict_cc_num[X_train_ultra_simple.iat[i,1]]\n",
    "        time_val.append(time.time() - start);start = time.time()\n",
    "        i=i+1\n",
    "    return time_val\n",
    "\n",
    "\n",
    "def ditc_maping(X_train_ultra_simple,dict_merchants_cc_num):\n",
    "    X_train_ultra_simple[\"merchant\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "    X_train_ultra_simple[\"cc_num\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "\n",
    "def ditc_maping_2(X_train_ultra_simple,dict_merchants_cc_num):\n",
    "    X_train_ultra_simple[\"merchant\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "    X_train_ultra_simple[\"cc_num\"].replace(dict_merchants_cc_num, inplace=True)\n",
    "\n",
    "    \n",
    "def create_split_dict(X_train_ultra_simple):\n",
    "    start = time.time()\n",
    "    dict_merchants=dict()\n",
    "    dict_cc_num=dict()\n",
    "    index =0\n",
    "    merc_id=0\n",
    "    cc_id=0\n",
    "    while index < len(X_train_ultra_simple):\n",
    "        if X_train_ultra_simple[\"merchant\"][index] not in dict_merchants.keys():\n",
    "            dict_merchants[X_train_ultra_simple[\"merchant\"][index]] = merc_id\n",
    "            merc_id=merc_id+1\n",
    "        index=index+1\n",
    "\n",
    "    print(\"---remplissage dict_merchants  %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    index=0\n",
    "    while index < len(X_train_ultra_simple):\n",
    "        if X_train_ultra_simple[\"cc_num\"][index] not in dict_cc_num.keys():\n",
    "            dict_cc_num[X_train_ultra_simple[\"cc_num\"][index]] = merc_id\n",
    "            merc_id=merc_id+1\n",
    "        index=index+1\n",
    "\n",
    "    print(\"---remplissage dict_cc_num %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return dict_merchants,dict_cc_num\n",
    "\n",
    "def create_dict(X_train_ultra_simple):\n",
    "    dict_merchants,dict_cc_num=create_split_dict(X_train_ultra_simple)\n",
    "    \n",
    "    return slow_concat(dict_merchants,dict_cc_num)\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "#########################################################################\n",
    "def create_dictionary_hell():# missing double values\n",
    "    start = time.time()\n",
    "    dictionary_hell=[]\n",
    "    nodes=[]\n",
    "    nb_sg=len(sous_graph)\n",
    "    print(len(sous_graph))\n",
    "    sg_index=0\n",
    "    merc_id=0\n",
    "    while sg_index < nb_sg:\n",
    "        i=0\n",
    "        merc_id=0\n",
    "        nodes= list(sous_graph[sg_index].nodes)\n",
    "        dictionary_hell.append(dict())\n",
    "        while( i<len(nodes)):\n",
    "            if nodes[i] not in dictionary_hell[sg_index].keys():\n",
    "                dictionary_hell[sg_index][nodes[i]] = merc_id\n",
    "                merc_id=merc_id+1\n",
    "                \n",
    "            i=i+1\n",
    "\n",
    "        sg_index=sg_index+1\n",
    "    print(\"--- dictionary__hell %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return dictionary_hell\n",
    "\n",
    "def relabel_graph(r_sous_graph,dictionary_hell,sous_graph):\n",
    "    print (\"relabel_\",end=\"\")\n",
    "    start = time.time()\n",
    "    i=0  \n",
    "    while i<len(sous_graph):\n",
    "        r_sous_graph.append(nx.relabel_nodes(sous_graph[i], dictionary_hell[i],copy=True))\n",
    "        i=i+1\n",
    "        print(\".\",end=\"\")\n",
    "    print(\"--- relabel_graph %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "def unrelabel_graph(unr_sous_graph,dictionary_hell,sous_graph):\n",
    "    start = time.time()\n",
    "    i=0\n",
    "    while i<len(dictionary_hell):\n",
    "        if i==len(sous_graph):\n",
    "            (print(\"errno=\",i,end=','))\n",
    "        inv_map = {v: k for k, v in dictionary_hell[i].items()}\n",
    "        unr_sous_graph.append(nx.relabel_nodes(sous_graph[i], inv_map,copy=True))\n",
    "        i=i+1\n",
    "    print(\"--- revert labelling %s seconds ---\" % (time.time() - start))\n",
    "\n",
    "\n",
    "def print_info_diverses(X_train_ultra_simple,dico):\n",
    "    print(X_train_ultra_simple[\"merchant\"][0])\n",
    "    print(X_train_ultra_simple[\"cc_num\"][0])\n",
    "    print(X_train_ultra_simple.columns)\n",
    "    print(X_train_ultra_simple.loc[0][\"merchant\"])\n",
    "\n",
    "    print (len(dico) , \"humans in the system \")#1676\n",
    "    print(X_train_ultra_simple[\"cc_num\"][0])\n",
    "    print (dict_merchants[ \"fraud_Rippin, Kub and Mann\"], type(dico[ \"fraud_Rippin, Kub and Mann\"]))\n",
    "    \n",
    "\n",
    "#laplacian similarity 1/2\n",
    "def select_k(spectrum, minimum_energy = 0.9):#\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)\n",
    "\n",
    "#laplacian similarity 2/2\n",
    "def laplacian_similarity(graph1,graph2):\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "    print(\"k selected =\",k)\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    return similarity\n",
    "\n",
    "def string_edit_dist():\n",
    "    print(\"https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Levenshtein.html\")\n",
    "def edit_dist_nx(g1,g2):\n",
    "    \n",
    "    for v in nx.optimize_graph_edit_distance(g1, g2):\n",
    "        minv = v\n",
    "    return minv\n",
    "\n",
    "\n",
    "def graph_degree(g):\n",
    "    degrees = [val for (node, val) in g.degree()]\n",
    "    maxd=max(degrees)\n",
    "    avg=sum(degrees)/len(degrees)\n",
    "    #print(\"max degree =\",maxd,\" mean = \",avg)\n",
    "    return maxd,avg\n",
    "def print_graph_info(connected_count):\n",
    "    \n",
    "    print(connected_count,\"connected graphs\")\n",
    "    print(len(sous_graph[0].edges),\" transactions \")\n",
    "    print(\"sous_graph[0][0] , type = \",type(sous_graph[0][0]),\"\\n\")\n",
    "    print(\"sous_graph[0].nodes , type = \",type(sous_graph[0].nodes),\"\\n\")\n",
    "    print(list(sous_graph[0].nodes))\n",
    "    print(\"------------\")\n",
    "    sub_g=0;node=0\n",
    "    dico=dict(sous_graph[sub_g][node])\n",
    "    for key in dico:\n",
    "         print(sous_graph[0][0][key],\"key = \",key)\n",
    "    print(dict(sous_graph[0][0]))\n",
    "    print(\"------------\")\n",
    "    print( type(sous_graph[0][0][693]))\n",
    "    print(sous_graph[0][0][693])\n",
    "    print(\"---------\")\n",
    "    print((sous_graph[0].edges))\n",
    "    \n",
    "def draw_1(g):\n",
    "    start = time.time()\n",
    "    #subax1 = plt.subplot()\n",
    "    nx.draw(sous_graph[0], with_labels=False, node_size= 1)\n",
    "    plt.savefig(\"draw_1.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "def draw_2(g):\n",
    "    start = time.time()\n",
    "    #subax2 = plt.subplot()\n",
    "    options = {\n",
    "        'node_size': 100,\n",
    "        'width': 3,\n",
    "    }\n",
    "    nx.draw_spectral(g, **options)#approximation of the ratio cut\n",
    "    plt.savefig(\"draw_2.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "def draw_3(g):\n",
    "    start = time.time()\n",
    "    #subax3 = plt.subplot()\n",
    "\n",
    "    nx.draw_shell(g, with_labels=False,node_size= 1)# font_weight='bold')\n",
    "    plt.savefig(\"draw_3.png\")\n",
    "    plt.show()\n",
    "    print(\"---draw  %s seconds ---\" % (time.time() - start));start = time.time\n",
    "    \n",
    "def draw_4(g,numb_merchant):\n",
    "    total=len(g.nodes)#les valeures ascossié ne sont pas les bonnes mais c'est \n",
    "    X = list(range(0,numb_merchant ))# juste pour la position geographique \n",
    "    Y= list(range(numb_merchant,total ))\n",
    "    pos = dict()\n",
    "    pos.update( (n, (1, i)) for i, n in enumerate(X) ) # put nodes from X at x=1\n",
    "    pos.update( (n, (2, i)) for i, n in enumerate(Y) ) # put nodes from Y at x=2\n",
    "    nx.draw(g, pos=pos,node_size= 1)\n",
    "    plt.savefig(\"draw_4.png\")\n",
    "    plt.show()\n",
    "\n",
    "def drawing(g):\n",
    "    %matplotlib inline\n",
    "    draw_1(g)\n",
    "    draw_2(g)\n",
    "    draw_3(g)\n",
    "    draw_4(g,len(dict_merchants.keys()))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def complexity_calculations():\n",
    "    val=0\n",
    "    train_size=len(Y_train)\n",
    "    test_size=len(Y_test)\n",
    "    \n",
    "    #l'idee premiere\n",
    "    print(\"-----------------\")\n",
    "    print(\"tester la similarité de 1 graph modifié avec tt les autres \")\n",
    "    print(\"pour 0.5 sec par calcul et \",num_frame,\" frames\" )\n",
    "    print( num_frame*0.5,\" sec\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"modifier le graph et recomencer , pour chaques valeures dans train\")\n",
    "    print(\"pour completement calculer  les similarité de 1 transaction\")\n",
    "    print( num_frame*0.5*train_size,\" sec\")\n",
    "    print(\"donc \",int(num_frame*0.5*train_size/(3600*24)),\" jours\")\n",
    "    print(\"-----------------\")\n",
    "    \n",
    "    big_number = num_frame*0.5*train_size*test_size\n",
    "    big_number_year=int(big_number/31540000)\n",
    "    print(\"pour un total de \",big_number,\" sec\")\n",
    "    print(\"donc \",big_number_year,\" ans\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"en reduisant la precision au minimum\")\n",
    "    print(\"chaque transaction n'aura que 1 calcul de similarité\")\n",
    "    print(\"precision max 50% , doubler le temps de calcul double la precision\")\n",
    "    big_number=0.5*test_size\n",
    "    print(\"pour un total de \",big_number,\" sec\")\n",
    "    print(\"donc \",int(big_number/3600),\" heures\")\n",
    "\n",
    "    \n",
    "def int_to_str(G):\n",
    "    # convert nodes from int to str format\n",
    "    keys = np.arange(0,int(len(dictionary.keys())))\n",
    "    values = [str(i) for i in keys]\n",
    "    dic = dict(zip(keys, values))\n",
    "    H = nx.relabel_nodes(G, dic)\n",
    "\n",
    "def try_catch_example():\n",
    "    print('How many cats do you have?\\n')\n",
    "    numCats = input()\n",
    "    try:\n",
    "        if int(numCats) > 3:\n",
    "            print('That is a lot of cats.')\n",
    "        else:\n",
    "            print('That is not that many cats.')\n",
    "    except ValueError:\n",
    "        print(\"Value error\")  \n",
    "def modif_graph_testing(graph):\n",
    "    graph[1][2][\"weight\"]=2\n",
    "    \n",
    "def invert_node_testing():\n",
    "    graph=nx.MultiGraph()\n",
    "    graph.add_nodes_from([0,1,2,3,4])\n",
    "    graph.add_edge(0,1,weight=0,edge_id=\"a\")\n",
    "    graph.add_edge(1,2,weight=0,edge_id=\"b\")\n",
    "    graph.add_edge(2,3,weight=0,edge_id=\"c\")\n",
    "    graph.add_edge(3,4,weight=0,edge_id=\"d\")\n",
    "    graph.add_edge(4,2,weight=0,edge_id=\"e\")\n",
    "    print(graph.nodes)\n",
    "    print(graph.edges)\n",
    "    print(\"avan 1--2--3--4\")\n",
    "    #new_graph=inver t_graph(graph)\n",
    "    print(\"apres 1--2--3--4\")\n",
    "    print(new_graph.nodes)\n",
    "    print(new_graph.edges)\n",
    "    \n",
    "    \n",
    "    \n",
    "def max_degree(inv_sg):\n",
    "    _max=0\n",
    "    for gr in inv_sg:\n",
    "        current_max,mean =graph_degree(gr)\n",
    "        if current_max>_max:\n",
    "            _max=current_max\n",
    "    print (\"max degree= \",_max)\n",
    "    return _max\n",
    "\n",
    "def print_version():\n",
    "    print (\"python \",sys.version)\n",
    "\n",
    "    \n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "def hope_on_inv_sg(inv_sg,dim=58,nb_frame=200):\n",
    "\n",
    "    start = time.time()\n",
    "    # hope un_attributed example\n",
    "    model = HOPE(dimensions=dim)\n",
    "    i=0\n",
    "    model.fit(inv_sg[i])\n",
    "    arr=model.get_embedding()\n",
    "    i=1\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=nb_frame)\n",
    "    while (i<nb_frame):\n",
    "        # train the model and generate embeddings\n",
    "        model.fit(inv_sg[i])\n",
    "        arr=np.concatenate((arr,model.get_embedding()),axis=0)\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---hope embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return arr  \n",
    "\n",
    "def deepwalk_on_inv_sg(inv_sg,dim=58,nb_frame=200):\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    model = DeepWalk(dimensions=dim)\n",
    "    i=0\n",
    "    model.fit(inv_sg[i])\n",
    "    arr=model.get_embedding()\n",
    "    i=1\n",
    "    process_bar = progressbar.ProgressBar().start(max_value=nb_frame)\n",
    "    while (i<nb_frame):\n",
    "        # train the model and generate embeddings\n",
    "        model.fit(inv_sg[i])\n",
    "        arr=np.concatenate((arr,model.get_embedding()),axis=0)\n",
    "        i+=1\n",
    "        process_bar.update(i)\n",
    "    print(\"---deepwalk embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "    return arr      \n",
    "\n",
    "\n",
    "\n",
    "print(\"fonctions declaré a :\",time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2996dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431325d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long']\n",
      "(1037340, 2)\n",
      "(555719, 2)\n"
     ]
    }
   ],
   "source": [
    "##download data\n",
    "\n",
    "program_start = time.time()\n",
    "\n",
    "import os\n",
    "data_file= os.path.abspath('../../data')\n",
    "full_path=data_file+'\\\\'+'fraudTrain.csv'\n",
    "train_df=pd.read_csv(full_path)\n",
    "full_path=data_file+'\\\\'+'fraudTest.csv'\n",
    "test_df=pd.read_csv(full_path)\n",
    "\n",
    "cols = train_df.columns.tolist()\n",
    "cols = [c for c in cols if c not in [\"is_fraud\"]]\n",
    "target = \"is_fraud\"\n",
    "print(cols)\n",
    "\n",
    "#Definition des nouvelles variables X_train and Y_train\n",
    "X_train = train_df[cols]\n",
    "Y_train = train_df[target]\n",
    "\n",
    "#Definition des nouvelles variables X_test and Y_test\n",
    "X_test = test_df[cols]\n",
    "Y_test = test_df[target]\n",
    "\n",
    "features = [ 'merchant', 'cc_num']\n",
    "X_train = X_train[features]\n",
    "X_test = X_test[features]\n",
    "\n",
    "X_train_ultra_simple = X_train.copy()\n",
    "X_train_ultra_simple=X_train_ultra_simple.iloc[:int(len(X_train_ultra_simple)*0.8)]\n",
    "X_test_ultra_simple = X_test.copy()\n",
    "num_frame_test=round((len(X_test_ultra_simple)/len(X_train_ultra_simple))*num_frame)\n",
    "#you need to make a smaller number of subgraph\n",
    "print(X_train_ultra_simple.shape)\n",
    "print(X_test_ultra_simple.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c91466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2882e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d79b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---remplissage dict_merchants  7.6128411293029785 seconds ---\n",
      "---remplissage dict_cc_num 7.593199253082275 seconds ---\n",
      "---remplissage dict_merchants  4.306015491485596 seconds ---\n",
      "---remplissage dict_cc_num 4.101031303405762 seconds ---\n",
      "---dictionary maping = 69.9957230091095 seconds ---\n",
      "---graph construction = 18.47732186317444 seconds ---\n",
      "---graph construction = 10.351585149765015 seconds ---\n",
      "200  sous graphs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#remplissage des dictionaires\n",
    "#_test\n",
    "dict_merchants,dict_cc_num=create_split_dict(X_train_ultra_simple)\n",
    "dict_merchants_test,dict_cc_num_test=create_split_dict(X_test_ultra_simple)\n",
    "dictionary=slow_concat(dict_merchants,dict_cc_num)\n",
    "dictionary_test=slow_concat(dict_merchants_test,dict_cc_num_test)\n",
    "#dictionary=create_dict(X_train_ultra_simple)\n",
    "\n",
    "# associer a chaque marchant son numero dans le dictionaire\n",
    "#pour la lisibilité , et l'affichage\n",
    "start = time.time()\n",
    "ditc_maping(X_train_ultra_simple,dictionary)\n",
    "ditc_maping(X_test_ultra_simple,dictionary_test)\n",
    "print(\"---dictionary maping = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "\n",
    "\n",
    "#pip install tk\n",
    "\n",
    "    ##creation du graph\n",
    "    \n",
    "\n",
    "g = create_graph(X_train_ultra_simple)#40 sec\n",
    "g_test= create_graph(X_test_ultra_simple)\n",
    "#!jupyter notebook --generate-config\n",
    "# divison en plusieures sous graphs #20 sec\n",
    "\n",
    "sous_graph,connected_count=create_sub_graph(X_train_ultra_simple,num_frame,dict_merchants,dict_cc_num)\n",
    "sous_graph_test,connected_count=create_sub_graph(X_test_ultra_simple,num_frame_test,dict_merchants,dict_cc_num)\n",
    "\n",
    "#dictionary_hell=create_dictionary_hell()\n",
    "\n",
    "#pour afficher , attention au cascades\n",
    "#!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "\n",
    "print(connected_count,\" connected graphs\")\n",
    "\n",
    "\n",
    "print(\"---depuis le debut  %s seconds ---\" % (time.time() - program_start));start = time.time\n",
    "#!git clone https://github.com/shenweichen/GraphEmbedding.git\n",
    "\n",
    "\n",
    "\n",
    "if(False):\n",
    "    liste=[]\n",
    "    i=0\n",
    "    num_frame=len(sous_graph)\n",
    "    G = sous_graph[i]\n",
    "    liste.append(encapsulation(G))\n",
    "    i+=1\n",
    "\n",
    "    G = sous_graph[i]\n",
    "    liste.append(encapsulation(G))\n",
    "    i+=1\n",
    "\n",
    "#crash_free()\n",
    "#!pip3 install PyQt5\n",
    "# affichage\n",
    "print(sous_graph[0])\n",
    "#drawing(sous_graph[0])\n",
    "\n",
    "print(len(sous_graph[0].nodes))\n",
    "print(len(g.nodes))\n",
    "\n",
    "\n",
    "#370 MiB = 300 000 Mo\n",
    "# avec g/200 ca compile , tres .... lentement , mais ca compile\n",
    "too_long=True\n",
    "if(not too_long):\n",
    "    val = edit_dist_nx(sous_graph[0],sous_graph[1])\n",
    "\n",
    "affichage=False\n",
    "if(affichage):\n",
    "    complexity_calculations()\n",
    "    drawing(g)\n",
    "    print_graph_info(connected_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb73f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging=False\n",
    "if (debugging):\n",
    "    sous_graph,connected_count=create_sub_graph(X_train_ultra_simple,num_frame,dict_merchants,dict_cc_num)\n",
    "    sous_graph_test,connected_count=create_sub_graph(X_test_ultra_simple,num_frame_test,dict_merchants,dict_cc_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b81799",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(num_frame)\n",
    "    print(len(sous_graph))\n",
    "    print(len(sous_graph_test))\n",
    "    print(num_frame_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6a91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### partie karateclub\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph construction\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "inv_sg = create_inverted_sub_graph(sous_graph)\n",
    "inv_sg_test= create_inverted_sub_graph(sous_graph_test)\n",
    "print(\"--- graph inversion = %s seconds ---\" % int(time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06d26a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def is_it_missing_a_number(whatever):\n",
    "    missing=False\n",
    "    if isinstance(whatever,dict):\n",
    "    \n",
    "        l=sorted(whatever.keys())\n",
    "        i=0\n",
    "        while i<len(l):\n",
    "            if l[i]!=i:\n",
    "                missing=True\n",
    "            i=i+1\n",
    "        i=i-1\n",
    "        #print(\"len(dict)= \",len(l),\" , lastval = \",l[i],end=\" \")\n",
    "        if(missing):\n",
    "            print(\" missing a number\",end=\" \")\n",
    "            return True\n",
    "    if isinstance(whatever,list):\n",
    "        l=sorted(whatever)\n",
    "        i=0\n",
    "        while i<len(l):\n",
    "            if l[i]!=i:\n",
    "                missing=True\n",
    "            i=i+1\n",
    "        i=i-1\n",
    "        #print(\"len(list)= \",len(l),\" , lastval = \",l[i],end=\" \")\n",
    "        if(missing):\n",
    "            print(\" missing a number\",end=\" \")\n",
    "            return True\n",
    "    #print(\"\\n\")\n",
    "    return False\n",
    "    \n",
    "def is_inv_working():\n",
    "    print (len(inv_sg[30].nodes))\n",
    "    l1=list(inv_sg[30].nodes)\n",
    "    l2=list(unr_sous_graph[30].nodes)\n",
    "    i=0\n",
    "    while(i<len( inv_sg[30])):\n",
    "        if(l1[i] != l2[i]):\n",
    "            print(\"not_the_same\")\n",
    "        i=i+1\n",
    "\n",
    "    i=0# il y a au moins les memes values\n",
    "    for k in inv_sg[30] :\n",
    "        #print(\",\",end=\"\");i=i+1\n",
    "        if k not in(unr_sous_graph[30] ):\n",
    "            print(\"not_the_same\")\n",
    "    print(i)\n",
    "    \n",
    "def is_inv_conservating_order(inv_sg):\n",
    "    node_list=[]\n",
    "    for node in inv_sg.nodes(data=True):\n",
    "        \n",
    "        node_list.append([node[0],node[1][\"edge_id\"]])\n",
    "        #[inv_sg.nodes(node,data=\"node_id\"),inv_sg.nodes(node,data=\"edge_id\")]\n",
    "    #print (np.array(node_list))\n",
    "    return (np.array(node_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead262ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list=is_inv_conservating_order(inv_sg[10])\n",
    "node_list.sort(axis=0)\n",
    "print(node_list)\n",
    "md=max_degree(inv_sg)\n",
    "md2=max_degree(inv_sg_test)\n",
    "print (len(inv_sg))\n",
    "print (len(inv_sg_test))#should be 107\n",
    "print(\"len inv_sg = \",len(inv_sg),len(inv_sg)*len(inv_sg[0]),\" / len xtrain = \",len(X_train_ultra_simple))\n",
    "print(\"len inv_sg_test = \",len(inv_sg_test),\" / len xtest = \",len(X_test_ultra_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403819e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71190ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(list(r_sous_graph[0].nodes)))\n",
    "#print(sorted((dictionary_hell[0]).keys()))\n",
    "#is_it_missing_a_number(dictionary_hell[1])\n",
    "i=0\n",
    "while(i<len(inv_sg)):\n",
    "    nodes = inv_sg[i].nodes(data=False)\n",
    "    is_it_missing_a_number(list(nodes))\n",
    "    i=i+1\n",
    "# Graph2Vec attributed example\n",
    "if(False):\n",
    "    small_sous_graph=[r_sous_graph[0],r_sous_graph[1]]\n",
    "    \n",
    "    model = Graph2Vec(attributed=False)\n",
    "    model.fit(small_sous_graph)\n",
    "    ecmb=model.get_embedding()\n",
    "\n",
    "    print(type(range(r_sous_graph[0].number_of_nodes())))\n",
    "\n",
    "    print(\"---embedding = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76816aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---deepwalk embedding = 6973.2731170654297 seconds ---\n",
    "md=max_degree(inv_sg)\n",
    "hope_emb = deepwalk_on_inv_sg(inv_sg,dim=md,nb_frame=num_frame)\n",
    "hope_emb_test = deepwalk_on_inv_sg(inv_sg_test,dim=md,nb_frame=num_frame_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96586af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e224bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---deepwalk embedding = 6973.2731170654297 seconds ---\n",
    "#deepwalk_emb=deepwalk_on_inv_sg(inv_sg)\n",
    "#deepwalk_emb=deepwalk_on_inv_sg(inv_sg,max_degree(inv_sg))# variable size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675a616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613f992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e461b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ded700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- fin de la partie graph\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "################################################################\n",
    "#-----------------   debut de la partie IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8f758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import le data_set complet pour xgboost et cie\n",
    "full_path=data_file+'\\\\'+'X_train_1_2_svm.csv'\n",
    "xtrain_transformed_complique=pd.read_csv(full_path)\n",
    "ytrain_transformed_complique=train_df['is_fraud'].iloc[:int(len(train_df)*0.8)]\n",
    "\n",
    "full_path=data_file+'\\\\'+'X_test_1_2_svm.csv'\n",
    "xtest_transformed_complique=pd.read_csv(full_path)\n",
    "ytest_transformed_complique=test_df['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526d6d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add columns\n",
    "print((hope_emb.shape))\n",
    "print(xtrain_transformed_complique.shape)\n",
    "hope_xtrain_concat=np.concatenate((xtrain_transformed_complique,hope_emb),axis=1)\n",
    "hope_xtest_concat=np.concatenate((xtest_transformed_complique,hope_emb_test),axis=1)\n",
    "\n",
    "print(hope_xtrain_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test avec xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sans modification\n",
    "start = time.time()\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=50, gamma=0.05,eta=0.05,max_depth=7, n_jobs=16)\n",
    "\n",
    "xgb.fit(xtrain_transformed_complique,ytrain_transformed_complique)\n",
    "Y_train_pred=xgb.predict(xtrain_transformed_complique)\n",
    "Y_test_pred=xgb.predict(xtest_transformed_complique)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred))\n",
    "print(classification_report(Y_test,Y_test_pred))\n",
    "\n",
    "print(\"---XGboost sans modification = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout du graph\n",
    "\n",
    "xgb.fit(hope_xtrain_concat,ytrain_transformed_complique)\n",
    "Y_train_pred=xgb.predict(hope_xtrain_concat)\n",
    "Y_test_pred=xgb.predict(hope_xtest_concat)\n",
    "print(classification_report(ytrain_transformed_complique,Y_train_pred))\n",
    "print(classification_report(Y_test,Y_test_pred))\n",
    "\n",
    "print(\"---XGboost avec modification = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "if(False):\n",
    "    start = time.time()\n",
    "\n",
    "    explainer = shap.TreeExplainer(xgb) \n",
    "    shap_values = explainer.shap_values(hope_xtest_concat) \n",
    "    shap.summary_plot(shap_values, hope_xtest_concat, plot_type=\"bar\")\n",
    "\n",
    "    print(\"---shap for XGboost = %s seconds ---\" % (time.time() - start));start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8022c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=classification_report(ytrain_transformed_complique,Y_train_pred,output_dict=True)\n",
    "cl2=classification_report(Y_test,Y_test_pred,output_dict=True)\n",
    "print(\"recall diff =\",cl[\"1\"][\"recall\"]-cl2[\"1\"][\"recall\"])\n",
    "print(\"f1-score diff =\",cl[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ytrain_transformed_complique.shape)\n",
    "print(Y_train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "svm_model = svm.SVC(kernel=\"rbf\", gamma = 0.02, C=10)\n",
    "svm_model.fit(xtrain_transformed_complique,Y_train_pred)\n",
    "Y_train_pred=xgb.predict(xtrain_transformed_complique)\n",
    "Y_test_pred=xgb.predict(xtest_transformed_complique)\n",
    "print(\"---SVM sans modification = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "cl=classification_report(ytrain_transformed_complique,Y_train_pred,output_dict=True)\n",
    "cl2=classification_report(Y_test,Y_test_pred,output_dict=True)\n",
    "print(\"recall diff =\",cl[\"1\"][\"recall\"]-cl2[\"1\"][\"recall\"])\n",
    "print(\"f1-score diff =\",cl[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "print(\"--------------------------------------------------------\")\n",
    "#hope_xtrain_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d568b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(hope_xtrain_concat,Y_train_pred)\n",
    "Y_train_pred=xgb.predict(hope_xtrain_concat)\n",
    "Y_test_pred=xgb.predict(hope_xtest_concat)\n",
    "print(\"---SVM sans modification = %s seconds ---\" % (time.time() - start));start = time.time()\n",
    "cl=classification_report(ytrain_transformed_complique,Y_train_pred,output_dict=True)\n",
    "cl2=classification_report(Y_test,Y_test_pred,output_dict=True)\n",
    "print(\"recall diff =\",cl[\"1\"][\"recall\"]-cl2[\"1\"][\"recall\"])\n",
    "print(\"f1-score diff =\",cl[\"1\"][\"f1-score\"]-cl[\"1\"][\"f1-score\"])\n",
    "print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
