{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD PACKAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score,classification_report,roc_auc_score,precision_score,recall_score, precision_recall_curve, auc, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Feature engineering.ipynb\" import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Lire la data stock√©e dans le fichier CSV. Modifier le chemin si vous relancer le kernel \n",
    "# # #df=pd.read_csv(r'C:\\Users\\anas.sory\\Documents\\Stage\\creditcard.csv')\n",
    "# train_df=pd.read_csv(r'C:\\Users\\MohamedAliELBOURI\\Desktop\\Projet\\fraude-main\\fraude-main\\Data\\archive fraude CB7\\fraudTrain.csv')\n",
    "# test_df=pd.read_csv(r'C:\\Users\\MohamedAliELBOURI\\Desktop\\Projet\\fraude-main\\fraude-main\\Data\\archive fraude CB7\\fraudTest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK THE DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = train_df.columns.tolist()\n",
    "# cols = [c for c in cols if c not in [\"is_fraud\"]]\n",
    "# target = \"is_fraud\"\n",
    "# print(cols)\n",
    "\n",
    "# #Definition des nouvelles variables X_train and Y_train, X_val and Y_val\n",
    "# X_train = train_df[cols]\n",
    "# Y_train = train_df[target]\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "# #Definition des nouvelles variables X_test and Y_test\n",
    "# X_test = test_df[cols]\n",
    "# Y_test = test_df[target]\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config1_1(X_train, X_test, cat_columns = ['category', 'state']):\n",
    "\n",
    "    X_train_1_1, X_test_1_1 = X_train.copy(), X_test.copy()\n",
    "    # We label encode 'merchant'\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train_1_1['merchant'])\n",
    "    X_train_1_1['merchant'] = le.transform(X_train_1_1['merchant'])\n",
    "    X_test_1_1['merchant'] = le.transform(X_test_1_1['merchant'])\n",
    "\n",
    "    # Instantiate encoder/scaler\n",
    "    scaler = StandardScaler()\n",
    "    ohe    = OneHotEncoder(sparse=False, handle_unknown = 'ignore')\n",
    "\n",
    "    # We fit the transformers on the Train data\n",
    "    num_columns = X_train_1_1.select_dtypes(include=np.number).columns.tolist()  \n",
    "    scaler.fit(X_train_1_1[num_columns])\n",
    "    ohe.fit(X_train_1_1[cat_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_1_1[num_columns]) \n",
    "    encoded_columns_train =    ohe.transform(X_train_1_1[cat_columns])\n",
    "\n",
    "    scaled_columns_test  = scaler.transform(X_test_1_1[num_columns]) \n",
    "    encoded_columns_test =    ohe.transform(X_test_1_1[cat_columns])\n",
    "\n",
    "    # Concatenate (Column-Bind) Processed Columns Back Together\n",
    "    X_train_ohe_std = np.concatenate([scaled_columns_train, encoded_columns_train], axis=1)\n",
    "    X_test_ohe_std = np.concatenate([scaled_columns_test, encoded_columns_test], axis=1)\n",
    "    ohe_std_cols = np.concatenate([num_columns, ohe.get_feature_names()])\n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_ohe_std = pd.DataFrame(X_train_ohe_std, index=X_train.index, columns=ohe_std_cols)\n",
    "    X_test_ohe_std = pd.DataFrame(X_test_ohe_std, index=X_test.index, columns=ohe_std_cols)\n",
    "\n",
    "    return X_train_ohe_std, X_test_ohe_std\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config1_2(X_train, X_test, cat_columns = ['category', 'state', 'merchant']):\n",
    "    X_train_1_2, X_test_1_2 = X_train.copy(), X_test.copy()\n",
    "    \n",
    "    # Instantiate scaler/labelEncoder\"\n",
    "    scaler = StandardScaler()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    #We transform categorical variables to integers\n",
    "    for cat in cat_columns:\n",
    "        le.fit(X_train[cat])\n",
    "        X_train_1_2[cat] = le.transform(X_train_1_2[cat])\n",
    "        X_test_1_2[cat] = le.transform(X_test_1_2[cat])\n",
    "\n",
    "    # We fit the scaler on all numerical features (transformed cat_columns are included)\n",
    "    num_columns = X_train_1_2.select_dtypes(include=np.number).columns.tolist()  \n",
    "    scaler.fit(X_train_1_2[num_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_1_2[num_columns]) \n",
    "    scaled_columns_test  = scaler.transform(X_test_1_2[num_columns]) \n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_le_std = pd.DataFrame(scaled_columns_train, index=X_train.index, columns=num_columns)\n",
    "    X_test_le_std = pd.DataFrame(scaled_columns_test, index=X_test.index, columns=num_columns)\n",
    "\n",
    "    return X_train_le_std, X_test_le_std\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def config2_1(X_train, Y_train, X_test, cat_columns = ['category', 'state', 'merchant']):\n",
    "    # We define the numerical columns (transformed cat_columns are not included)\n",
    "    num_columns = X_train.select_dtypes(include=np.number).columns.tolist()  \n",
    "\n",
    "    X_train_2_1, X_test_2_1 = X_train.copy(), X_test.copy()\n",
    "    for feature in cat_columns:\n",
    "        categories, replace_list = fraud_risk_percentile_mapping(X_train, Y_train, feature)\n",
    "        X_train_2_1[feature] = X_train_2_1[feature].replace(categories,replace_list)\n",
    "        X_test_2_1[feature] = X_test_2_1[feature].replace(categories,replace_list)\n",
    "\n",
    "\n",
    "    # Instantiate encoder/scaler\n",
    "    scaler = StandardScaler()\n",
    "    ohe    = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "    # We fit the transformers on the Train data\n",
    "    scaler.fit(X_train_2_1[num_columns])\n",
    "    ohe.fit(X_train_2_1[cat_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_2_1[num_columns]) \n",
    "    encoded_columns_train =    ohe.transform(X_train_2_1[cat_columns])\n",
    "\n",
    "    scaled_columns_test  = scaler.transform(X_test_2_1[num_columns]) \n",
    "    encoded_columns_test =    ohe.transform(X_test_2_1[cat_columns])\n",
    "\n",
    "    # Concatenate (Column-Bind) Processed Columns Back Together\n",
    "    X_train_ohe_std = np.concatenate([scaled_columns_train, encoded_columns_train], axis=1)\n",
    "    X_test_ohe_std = np.concatenate([scaled_columns_test, encoded_columns_test], axis=1)\n",
    "    ohe_std_cols = np.concatenate([num_columns, ohe.get_feature_names()])\n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_ohe_std = pd.DataFrame(X_train_ohe_std, index=X_train.index, columns=ohe_std_cols)\n",
    "    X_test_ohe_std = pd.DataFrame(X_test_ohe_std, index=X_test.index, columns=ohe_std_cols)\n",
    "\n",
    "    return X_train_ohe_std, X_test_ohe_std\n",
    "# X_train_2_1, X_val_2_1 = config2_1(X_train, Y_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config2_2(X_train, Y_train, X_test, cat_columns = ['category', 'state', 'merchant']):\n",
    "    X_train_2_1, X_test_2_1 = X_train.copy(), X_test.copy()\n",
    "    for feature in cat_columns:\n",
    "        categories, replace_list = fraud_risk_percentile_mapping(X_train, Y_train, feature)\n",
    "        X_train_2_1[feature] = X_train_2_1[feature].replace(categories,replace_list)\n",
    "        X_test_2_1[feature] = X_test_2_1[feature].replace(categories,replace_list)\n",
    "\n",
    "    # Instantiate scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # We fit the scaler on all numerical features (transformed cat_columns are included)\n",
    "    num_columns = X_train_2_1.select_dtypes(include=np.number).columns.tolist()  \n",
    "    scaler.fit(X_train_2_1[num_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_2_1[num_columns]) \n",
    "    scaled_columns_test  = scaler.transform(X_test_2_1[num_columns]) \n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_le_std = pd.DataFrame(scaled_columns_train, index=X_train.index, columns=num_columns)\n",
    "    X_test_le_std = pd.DataFrame(scaled_columns_test, index=X_test.index, columns=num_columns)\n",
    "\n",
    "    return X_train_le_std, X_test_le_std\n",
    "    \n",
    "# X_train_2_2, X_val_2_2 = config2_2(X_train, Y_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config3_1(X_train, X_test, cat_columns = ['category', 'state']):\n",
    "    X_train_3_1, X_test_3_1 = X_train.copy(), X_test.copy()\n",
    "    # We label encode 'merchant'\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train_3_1['merchant'])\n",
    "    X_train_3_1['merchant'] = le.transform(X_train_3_1['merchant'])\n",
    "    X_test_3_1['merchant'] = le.transform(X_test_3_1['merchant'])\n",
    "\n",
    "    # We define the numerical columns (including merchant)\n",
    "    num_columns = X_train_3_1.select_dtypes(include=np.number).columns.tolist()  \n",
    "\n",
    "    # We transform 'category' to label encoding (ordered by necessity of the transaction)\n",
    "    X_train_3_1['category'], X_test_3_1['category'] = X_transform_transaction_category(X_train, X_test)\n",
    "\n",
    "    # We transform states to regions (51 states to 5 geographic regions)\n",
    "    states, regions = state_to_region(X_train_3_1)\n",
    "    X_train_3_1['state'] = X_train_3_1['state'].replace(states, regions)\n",
    "    X_test_3_1['state'] = X_test_3_1['state'].replace(states, regions)\n",
    "\n",
    "    # Instantiate scaler/encoder\n",
    "    scaler = StandardScaler()\n",
    "    ohe    = OneHotEncoder(sparse=False)\n",
    "\n",
    "    # We fit the transformers on the Train data\n",
    "    scaler.fit(X_train_3_1[num_columns])\n",
    "    ohe.fit(X_train_3_1[cat_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_3_1[num_columns]) \n",
    "    encoded_columns_train =    ohe.transform(X_train_3_1[cat_columns])\n",
    "\n",
    "    scaled_columns_test  = scaler.transform(X_test_3_1[num_columns]) \n",
    "    encoded_columns_test =    ohe.transform(X_test_3_1[cat_columns])\n",
    "\n",
    "    # Concatenate (Column-Bind) Processed Columns Back Together\n",
    "    X_train_ohe_std = np.concatenate([scaled_columns_train, encoded_columns_train], axis=1)\n",
    "    X_test_ohe_std = np.concatenate([scaled_columns_test, encoded_columns_test], axis=1)\n",
    "    ohe_std_cols = np.concatenate([num_columns, ohe.get_feature_names()])\n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_ohe_std = pd.DataFrame(X_train_ohe_std, index=X_train.index, columns=ohe_std_cols)\n",
    "    X_test_ohe_std = pd.DataFrame(X_test_ohe_std, index=X_test.index, columns=ohe_std_cols)\n",
    "\n",
    "    return X_train_ohe_std, X_test_ohe_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config3_2(X_train, X_test):\n",
    "    X_train_3_2, X_test_3_2 = X_train.copy(), X_test.copy()\n",
    "    # We label encode 'merchant'\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train_3_2['merchant'])\n",
    "    X_train_3_2['merchant'] = le.transform(X_train_3_2['merchant'])\n",
    "    X_test_3_2['merchant'] = le.transform(X_test_3_2['merchant'])\n",
    "\n",
    "    # We transform 'category' to label encoding (ordered by necessity of the transaction)\n",
    "    X_train_3_2['category'], X_test_3_2['category'] = X_transform_transaction_category(X_train, X_test)\n",
    "\n",
    "    # We transform states to regions (51 states to 5 geographic regions)\n",
    "    states, regions = state_to_region(X_train_3_2)\n",
    "    X_train_3_2['state'] = X_train_3_2['state'].replace(states, regions)\n",
    "    X_test_3_2['state'] = X_test_3_2['state'].replace(states, regions)\n",
    "    le.fit(X_train_3_2['state'])\n",
    "    X_train_3_2['state'] = le.transform(X_train_3_2['state'])\n",
    "    X_test_3_2['state'] = le.transform(X_test_3_2['state'])\n",
    "\n",
    "\n",
    "    # We define the numerical columns (including merchant, state and category)\n",
    "    num_columns = X_train_3_2.select_dtypes(include=np.number).columns.tolist()  \n",
    "\n",
    "\n",
    "    # Instantiate scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # We fit the transformers on the Train data\n",
    "    scaler.fit(X_train_3_2[num_columns])\n",
    "\n",
    "    # We apply the transformers on the Train and Test data\n",
    "    scaled_columns_train  = scaler.transform(X_train_3_2[num_columns]) \n",
    "    scaled_columns_test  = scaler.transform(X_test_3_2[num_columns]) \n",
    "\n",
    "    #Transforming both numpy array to dataframes\n",
    "    X_train_std = pd.DataFrame(scaled_columns_train, index=X_train.index, columns=num_columns)\n",
    "    X_test_std = pd.DataFrame(scaled_columns_test, index=X_test.index, columns=num_columns)\n",
    "\n",
    "    return X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataTransformer(X_train, X_val = None, X_test = None, Y_train = None, config_name='config1_1'):\n",
    "    # Defining copies for X\n",
    "    X_train_copy = X_train.copy()\n",
    "    X_val_copy = None\n",
    "    X_test_copy = None\n",
    "    if X_val is not None:\n",
    "        X_val_copy = X_val.copy()    \n",
    "    if X_test is not None:\n",
    "        X_test_copy = X_test.copy()\n",
    "\n",
    "    # Defining transformers:\n",
    "    scaler = StandardScaler()\n",
    "    ohe = OneHotEncoder()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Pre-processing for all configs:\n",
    "\n",
    "    # Changing Gender to binary, and age to numerical\n",
    "    X_train_copy['gender'] = X_train_copy['gender'].replace(['M','F'],[0,1])\n",
    "    X_train_copy['dob'] = np.floor((pd.to_datetime(X_train_copy['trans_date_trans_time']) - pd.to_datetime(X_train_copy['dob']))/np.timedelta64(1,'Y'))\n",
    "    if X_val_copy is not None:\n",
    "        X_val_copy['gender'] = X_val_copy['gender'].replace(['M','F'],[0,1])\n",
    "        X_val_copy['dob'] = np.floor((pd.to_datetime(X_val_copy['trans_date_trans_time']) - pd.to_datetime(X_val_copy['dob']))/np.timedelta64(1,'Y'))\n",
    "    if X_test_copy is not None:\n",
    "        X_test_copy['gender'] = X_test_copy['gender'].replace(['M','F'],[0,1])\n",
    "        X_test_copy['dob'] = np.floor((pd.to_datetime(X_test_copy['trans_date_trans_time']) - pd.to_datetime(X_test_copy['dob']))/np.timedelta64(1,'Y'))\n",
    "\n",
    "\n",
    "    # Removing Index column\n",
    "    X_train_copy = X_train_copy.drop('Unnamed: 0',axis=1)\n",
    "    if X_val_copy is not None:\n",
    "        X_val_copy = X_val_copy.drop('Unnamed: 0',axis=1)\n",
    "    if X_test_copy is not None:\n",
    "        X_test_copy = X_test_copy.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # Removing Credit Card Number\n",
    "    X_train_copy = X_train_copy.drop('cc_num',axis=1)\n",
    "    if X_val_copy is not None:\n",
    "        X_val_copy = X_val_copy.drop('cc_num',axis=1)\n",
    "    if X_test_copy is not None:\n",
    "        X_test_copy = X_test_copy.drop('cc_num',axis=1)\n",
    "\n",
    "    # Removing trans_date_trans_time column\n",
    "    X_train_copy = X_train_copy.drop('trans_date_trans_time',axis=1)\n",
    "    if X_val_copy is not None:\n",
    "        X_val_copy = X_val_copy.drop('trans_date_trans_time',axis=1)\n",
    "    if X_test_copy is not None:\n",
    "        X_test_copy = X_test_copy.drop('trans_date_trans_time',axis=1)\n",
    "    \n",
    "    # Removing trans_num column\n",
    "    X_train_copy = X_train_copy.drop('trans_num',axis=1)\n",
    "    if X_val_copy is not None:\n",
    "        X_val_copy = X_val_copy.drop('trans_num',axis=1)\n",
    "    if X_test_copy is not None:\n",
    "        X_test_copy = X_test_copy.drop('trans_num',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # No feature selection, no pre-preprocessing, labelEncoding categorical  variables and standard scaling numerical variables\n",
    "    if config_name == \"config0\": \n",
    "        # Defining numerical and categorical features\n",
    "        num_columns = X_train_copy.select_dtypes(include=np.number).columns.tolist() \n",
    "        cat_columns = X_train_copy.select_dtypes(include=np.object).columns.tolist()   \n",
    "        # fitting the scaler\n",
    "        scaler.fit(X_train_copy[num_columns])\n",
    "        X_train_copy[num_columns] = scaler.transform(X_train_copy[num_columns])\n",
    "        if X_val_copy is not None:\n",
    "            X_val_copy[num_columns] = scaler.transform(X_val_copy[num_columns])\n",
    "        if X_test_copy is not None:\n",
    "            X_test_copy[num_columns] = scaler.transform(X_test_copy[num_columns])\n",
    "\n",
    "        # Label encoding columns\n",
    "        for col in cat_columns:\n",
    "            # print(col)\n",
    "            # Fitting the label Encoder\n",
    "            le.fit(X_train_copy[col])\n",
    "            # Handling unkown values\n",
    "            for x in(X_val_copy, X_test_copy):\n",
    "                if X_val_copy is not None:\n",
    "                    unknown_values = list(set(X_val_copy[col].unique()) - set(le.classes_))\n",
    "                    # print(unknown_values)\n",
    "                    unk_replace = ['<unknown>'] * len(unknown_values)\n",
    "                    X_val_copy[col] = X_val_copy[col].replace(unknown_values, unk_replace)                \n",
    "                if X_test_copy is not None:\n",
    "                    unknown_values = list(set(X_val_copy[col].unique()) - set(le.classes_))\n",
    "                    # print(unknown_values)\n",
    "                    unk_replace = ['<unknown>'] * len(unknown_values)\n",
    "                    X_test_copy[col] = X_test_copy[col].replace(unknown_values, unk_replace)                \n",
    "                    \n",
    "            le.classes_ = np.append(le.classes_, '<unknown>')\n",
    "            # print(le.classes_)\n",
    "            # Transforming the categorical variable\n",
    "            \n",
    "            X_train_copy[col] = le.transform(X_train_copy[col])\n",
    "\n",
    "            if X_val_copy is not None:\n",
    "                X_val_copy[col] = le.transform(X_val_copy[col])\n",
    "                                \n",
    "            if X_test_copy is not None:\n",
    "                X_test_copy[col] = le.transform(X_test_copy[col])    \n",
    "\n",
    "        output = [X_train_copy]\n",
    "        if X_val_copy is not None:\n",
    "            output.append(X_val_copy)\n",
    "                            \n",
    "        if X_test_copy is not None:\n",
    "            output.append(X_test_copy)     \n",
    "        \n",
    "        return tuple(output)\n",
    "\n",
    "    else:\n",
    "        name_to_config = {\n",
    "            'config1_1':config1_1,\n",
    "            'config1_2':config1_2,\n",
    "            'config2_1':config2_1,\n",
    "            'config2_2':config2_2,\n",
    "            'config3_1':config3_1,\n",
    "            'config3_2':config3_2,\n",
    "        }\n",
    "        requires_y = {\n",
    "            'config1_1':False, 'config1_2':False, \n",
    "            'config2_1':True, 'config2_2':True,\n",
    "            'config3_1':False, 'config3_2':False\n",
    "        }\n",
    "        config = name_to_config[config_name]\n",
    "        output = []\n",
    "        if requires_y[config_name]:\n",
    "            if X_val_copy is not None:\n",
    "                X_train_conf,X_val_conf = config(X_train_copy, Y_train, X_val_copy)\n",
    "                output = [X_train_conf, X_val_conf]\n",
    "            if X_test_copy is not None:\n",
    "                X_train_conf,X_test_conf = config(X_train_copy, Y_train,X_test_copy)\n",
    "                if output:\n",
    "                    output.append(X_test_conf)\n",
    "                else:\n",
    "                    output = [X_train_conf, X_test_conf]\n",
    "        else:\n",
    "            if X_val_copy is not None:\n",
    "                X_train_conf,X_val_conf = config(X_train_copy, X_val_copy)\n",
    "                output = [X_train_conf, X_val_conf]\n",
    "            if X_test_copy is not None:\n",
    "                X_train_conf,X_test_conf = config(X_train_copy,X_test_copy)\n",
    "                if output:\n",
    "                    output.append(X_test_conf)\n",
    "                else:\n",
    "                    output = [X_train_conf, X_test_conf]\n",
    "        return tuple(output)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x1,x2=dataTransformer(X_train, X_val, X_test=None, Y_train=Y_train, config_name='config0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1.describe())\n",
    "# print(x2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1,x2=dataTransformer(X_train, X_val, X_test=None, Y_train=Y_train, config_name='config1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x1.describe())\n",
    "# print(x2.head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "335a2a0d9a71700e98e650d2ddf387756b21267f06e5b50443b6634b542591a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a2e8bccd30f11d43abe4a6bbb42bb08ce61d36435e22e6a20e0dd3e0014b7524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
